[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "MP01 ‚Äì Netflix",
    "section": "",
    "text": "Code\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n  dir.create(file.path(\"data\", \"mp01\"), recursive = TRUE, showWarnings = FALSE)\n}\n\nGLOBAL_TOP_10_FILENAME  &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n  download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                destfile = GLOBAL_TOP_10_FILENAME)\n}\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n  download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                destfile = COUNTRY_TOP_10_FILENAME)\n}\nCode\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n\nLoading required package: tidyverse\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nif(!require(\"janitor\"))   install.packages(\"janitor\")\n\n\nLoading required package: janitor\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nCode\nif(!require(\"gt\"))        install.packages(\"gt\")\n\n\nLoading required package: gt\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(ggplot2)\nlibrary(gt)\nCode\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME, show_col_types = FALSE) |&gt; \n  clean_names() |&gt; \n  mutate(season_title = if_else(season_title == \"N/A\", NA_character_, season_title))\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, show_col_types = FALSE) |&gt; \n  clean_names()\n\ncat(\"Global rows:\", nrow(GLOBAL_TOP_10), \"  Country rows:\", nrow(COUNTRY_TOP_10), \"\\n\")\n\n\nGlobal rows: 8880   Country rows: 413620"
  },
  {
    "objectID": "mp01.html#initial-data-exploration",
    "href": "mp01.html#initial-data-exploration",
    "title": "MP01 ‚Äì Netflix",
    "section": "Initial Data Exploration",
    "text": "Initial Data Exploration\nIn this section, we perform a first exploration of the global Netflix Top 10 dataset. Our goal is to understand the size of the dataset, the distribution of categories, and the diversity of programming represented.\n\n\nCode\n# total\ntotal_rows   &lt;- nrow(GLOBAL_TOP_10)\nunique_shows &lt;- n_distinct(GLOBAL_TOP_10$show_title)\n\n# category\ncategory_summary &lt;- GLOBAL_TOP_10 %&gt;%\n  count(category, sort = TRUE)\n\ncategory_summary %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Table 1: Number of Records by Category in Global Top 10\") %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#ffe5e5\"),\n    locations = cells_body(rows = category == \"TV (English)\")\n  )\n\n\n\n\n\n\n\n\nTable 1: Number of Records by Category in Global Top 10\n\n\ncategory\nn\n\n\n\n\nFilms (English)\n2220\n\n\nFilms (Non-English)\n2220\n\n\nTV (English)\n2220\n\n\nTV (Non-English)\n2220"
  },
  {
    "objectID": "mp01.html#interpretation",
    "href": "mp01.html#interpretation",
    "title": "MP01 ‚Äì Netflix",
    "section": "Interpretation:",
    "text": "Interpretation:\nThe dataset contains¬†8880¬†records¬†covering¬†2852¬†unique shows. Each of the four categories is evenly represented (English / Non-English TV & Films), ensuring comparability, though real-world popularity may concentrate in fewer titles."
  },
  {
    "objectID": "mp01.html#press-release-1-stranger-things-season-5",
    "href": "mp01.html#press-release-1-stranger-things-season-5",
    "title": "MP01 ‚Äì Netflix",
    "section": "Press Release 1: Stranger Things Season 5",
    "text": "Press Release 1: Stranger Things Season 5\n\n\nCode\nstranger_things &lt;- GLOBAL_TOP_10 %&gt;% filter(grepl(\"Stranger Things\", show_title))\n\ntotal_hours   &lt;- sum(stranger_things$weekly_hours_viewed, na.rm = TRUE)\nweeks_in_top10 &lt;- n_distinct(stranger_things$week)\n\nst_countries &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(grepl(\"Stranger Things\", show_title)) %&gt;%\n  summarise(num_countries = n_distinct(country_name)) %&gt;%\n  pull(num_countries)\n\ntop_english_tv &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(category == \"TV (English)\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_hours)) %&gt;%\n  slice(1:5)\n\n\n\nüì¢¬†Netflix‚Äôs Global Phenomenon Returns: Stranger Things Prepares for a Historic Final Season\n\nTotal hours viewed:¬†2,967,980,000\nWeeks in Top 10:¬†20\nCountries appeared:¬†93\n\nCompared with other English TV shows (like¬†Wednesday¬†and¬†Bridgerton), Stranger Things remains one of the strongest titles worldwide.\n\n\nCode\nst_data &lt;- stranger_things %&gt;%\n  group_by(week) %&gt;%\n  summarise(weekly_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nggplot(st_data, aes(x = week, y = weekly_hours/1e6)) +\n  geom_line(color = \"#e50914\", size = 1.2) +\n  geom_point(color = \"#e50914\") +\n  labs(title = \"Stranger Things Global Viewership Trend\",\n       x = \"Week\", y = \"Weekly Hours Viewed (Millions)\") +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead."
  },
  {
    "objectID": "mp01.html#press-release-2-netflixs-blockbuster-growth-in-india",
    "href": "mp01.html#press-release-2-netflixs-blockbuster-growth-in-india",
    "title": "MP01 ‚Äì Netflix",
    "section": "Press Release 2: Netflix‚Äôs Blockbuster Growth in India",
    "text": "Press Release 2: Netflix‚Äôs Blockbuster Growth in India\n\n\nCode\nindia_top &lt;- COUNTRY_TOP_10 %&gt;% filter(country_name == \"India\")\nus_top    &lt;- COUNTRY_TOP_10 %&gt;% filter(country_name == \"United States\")\n\nindia_hours &lt;- GLOBAL_TOP_10 %&gt;% \n  filter(show_title %in% india_top$show_title) %&gt;% \n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE)) %&gt;% \n  pull(total_hours)\n\nindia_shows &lt;- n_distinct(india_top$show_title)\nindia_unique &lt;- setdiff(india_top$show_title, us_top$show_title)\n\n\n\nüì¢ Netflix Celebrates Record-Breaking Hindi Hits and Rapid Subscriber Growth in the World‚Äôs Largest Market\n\nTotal Hindi content hours:¬†104,909,000,000\nUnique Indian shows in Top 10:¬†1173\nLocal-only hits:¬†Mahavatar Narsimha,¬†Son of Sardaar 2\n\n\n\nCode\nindia_top10 &lt;- india_top %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(weeks_in_top10 = n_distinct(week)) %&gt;%\n  arrange(desc(weeks_in_top10)) %&gt;%\n  slice(1:10)\n\nggplot(india_top10, aes(x = reorder(show_title, weeks_in_top10), y = weeks_in_top10)) +\n  geom_col(fill = \"#E50914\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Top 10 Netflix Shows in India (Weeks in Top 10)\", x = \"Show\", y = \"Weeks\")"
  },
  {
    "objectID": "mp01.html#press-release-3-netflixs-global-non-english-tv-success",
    "href": "mp01.html#press-release-3-netflixs-global-non-english-tv-success",
    "title": "MP01 ‚Äì Netflix",
    "section": "Press Release 3: Netflix‚Äôs Global Non-English TV Success",
    "text": "Press Release 3: Netflix‚Äôs Global Non-English TV Success\n\n\nCode\nnon_english_tv &lt;- GLOBAL_TOP_10 %&gt;% filter(category == \"TV (Non-English)\")\n\nnon_eng_hours &lt;- sum(non_english_tv$weekly_hours_viewed, na.rm = TRUE)\nnon_eng_shows &lt;- n_distinct(non_english_tv$show_title)\n\ntop_non_eng &lt;- non_english_tv %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n            total_weeks = n_distinct(week)) %&gt;%\n  arrange(desc(total_hours)) %&gt;%\n  slice(1:5)\n\n\n\nüì¢ From Seoul to Madrid: Non-English Originals Reshape Netflix‚Äôs Global Top 10\n\nNon-English hours viewed:¬†44,714,410,000\nUnique Non-English shows:¬†530\nBreakout titles:¬†Squid Game,¬†Money Heist\n\n\n\nCode\ntv_compare &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(category %in% c(\"TV (English)\", \"TV (Non-English)\")) %&gt;%\n  group_by(category) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nggplot(tv_compare, aes(x = category, y = total_hours/1e9, fill = category)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_manual(values = c(\"#e50914\", \"#221f1f\")) +\n  labs(title = \"English vs Non-English TV (Billions of Hours)\",\n       x = \"\", y = \"Total Hours (Billions)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lab03.html",
    "href": "lab03.html",
    "title": "STA 9750 ‚Äì Lab 03",
    "section": "",
    "text": "This is the Lab 03 report covering Vectors, Review, Packages, Variables, Comments, Vector Types, Functions, Control Flow, and Programming Exercises."
  },
  {
    "objectID": "lab03.html#creating-vectors",
    "href": "lab03.html#creating-vectors",
    "title": "STA 9750 ‚Äì Lab 03",
    "section": "Creating vectors",
    "text": "Creating vectors\n\n\nCode\nx &lt;- c(1, 3, 5, 7)\ny &lt;- 2:6\nz &lt;- seq(0, 1, by = 0.2)\nx; y; z\n\n\n[1] 1 3 5 7\n\n\n[1] 2 3 4 5 6\n\n\n[1] 0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\nCode\nx + 1\n\n\n[1] 2 4 6 8\n\n\nCode\nx * 2\n\n\n[1]  2  6 10 14\n\n\nCode\nx + c(1, 2)   # Recycling rule\n\n\n[1] 2 5 6 9\n\n\n\n\nCode\na &lt;- 5; b &lt;- 2\na^b; a %% b; a %/% b\n\n\n[1] 25\n\n\n[1] 1\n\n\n[1] 2\n\n\n\n\nCode\nls()\n\n\n[1] \"a\" \"b\" \"x\" \"y\" \"z\"\n\n\nCode\nrm(b); ls()\n\n\n[1] \"a\" \"x\" \"y\" \"z\"\n\n\nCode\n# Use ?mean in the Console to open help; avoid putting ? in a rendered document.\n\n\n\n\nCode\nlibrary(nycflights13)\nlibrary(tidyverse)\n\n\n\n\nCode\n# This can be slow; comment it out if your network is slow.\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nNROW(available.packages())\n\n\n[1] 22808\n\n\n\n\nCode\nhead(flights, 10)\n\n\n# A tibble: 10 √ó 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ‚Ñπ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\nCode\nmy_apples &lt;- 5\nmy_oranges &lt;- 6\nmy_fruit &lt;- my_apples + my_oranges\nmy_fruit\n\n\n[1] 11\n\n\n\n\nCode\n# This is a comment\nx &lt;- 1  # Inline comment\n\n# This line will not run:\n# y &lt;- 2\n\n\n\n\nCode\nnum  &lt;- c(1.2, 3.5)\nint  &lt;- c(1L, 2L, 3L)\nchr  &lt;- c(\"a\", \"b\")\nlgc  &lt;- c(TRUE, FALSE, NA)\n\ntypeof(num); typeof(int); typeof(chr); typeof(lgc)\n\n\n[1] \"double\"\n\n\n[1] \"integer\"\n\n\n[1] \"character\"\n\n\n[1] \"logical\"\n\n\nCode\nclass(num)\n\n\n[1] \"numeric\"\n\n\nCode\nas.numeric(chr)   # Conversion fails, produces NA\n\n\n[1] NA NA\n\n\nCode\nis.na(lgc)\n\n\n[1] FALSE FALSE  TRUE\n\n\n\n\nCode\nx &lt;- c(1, 2, NA, 4)\nmean(x, na.rm = TRUE)\n\n\n[1] 2.333333\n\n\n\n\nCode\nfahrenheit_to_celsius &lt;- function(f) {\n  (f - 32) * 5/9\n}\nfahrenheit_to_celsius(77)\n\n\n[1] 25\n\n\n\n\nCode\nn &lt;- 5\nif (n %% 2 == 0) {\n  msg &lt;- \"even\"\n} else {\n  msg &lt;- \"odd\"\n}\nmsg\n\n\n[1] \"odd\"\n\n\n\n\nCode\nout &lt;- c()\nfor (i in 1:5) {\n  out[i] &lt;- i^2\n}\nout\n\n\n[1]  1  4  9 16 25\n\n\n\n\nCode\nx &lt;- -3:3\nifelse(x &gt;= 0, \"non-negative\", \"negative\")\n\n\n[1] \"negative\"     \"negative\"     \"negative\"     \"non-negative\" \"non-negative\"\n[6] \"non-negative\" \"non-negative\"\n\n\n\n\nCode\nv &lt;- c(2, 5, NA, 9, 11)\nmean(v, na.rm = TRUE)\n\n\n[1] 6.75\n\n\nCode\nsum(v, na.rm = TRUE)\n\n\n[1] 27\n\n\nCode\nlength(v)\n\n\n[1] 5\n\n\n\n\nCode\nc_to_f &lt;- function(c) {\n  (c * 9/5) + 32\n}\nc_to_f(0)\n\n\n[1] 32\n\n\nCode\nc_to_f(20)\n\n\n[1] 68\n\n\nCode\nc_to_f(100)\n\n\n[1] 212\n\n\n\n\nCode\nflights |&gt;\n  group_by(month) |&gt;\n  summarize(mean_dep = mean(dep_delay, na.rm = TRUE)) |&gt;\n  arrange(month)\n\n\n# A tibble: 12 √ó 2\n   month mean_dep\n   &lt;int&gt;    &lt;dbl&gt;\n 1     1    10.0 \n 2     2    10.8 \n 3     3    13.2 \n 4     4    13.9 \n 5     5    13.0 \n 6     6    20.8 \n 7     7    21.7 \n 8     8    12.6 \n 9     9     6.72\n10    10     6.24\n11    11     5.44\n12    12    16.6"
  },
  {
    "objectID": "lab02.html#new-element-two-columns",
    "href": "lab02.html#new-element-two-columns",
    "title": "Lab 02 Practice",
    "section": "NEW ELEMENT ‚Äî Two Columns",
    "text": "NEW ELEMENT ‚Äî Two Columns\n\n\nNotes\n\nData: txhousing\nCity: Houston\nMetric: yearly sales\n\n\n\n\n# A tibble: 6 √ó 3\n   year sales avg_price\n  &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1  2000 52459   153285.\n2  2001 53856   158590.\n3  2002 56563   167714.\n4  2003 60732   171537.\n5  2004 66979   175822.\n6  2005 72800   185497."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xiaolin Wu ‚Äî STA 9750 Website",
    "section": "",
    "text": "Welcome!\nHi, my name is Xiaolin Wu.\nThis website hosts my submission materials for STA 9750 at Baruch College.\nI am currently a graduate student with interests in data analytics and urban design.\nHere, you‚Äôll find my course projects, assignments, and reflections.\n\nüìÑ My Resume\n\nüìß Contact: xiaolin.wu1@baruchmail.cuny.edu\n\n\n\n\n\n\n\n\nLast Updated: Friday 10 03, 2025 at 22:33PM"
  },
  {
    "objectID": "Lab08.html",
    "href": "Lab08.html",
    "title": "Lab08",
    "section": "",
    "text": "Code\n# load packages\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(broom)\n\n# check data\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nglimpse(mtcars)\n\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,‚Ä¶\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,‚Ä¶\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16‚Ä¶\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180‚Ä¶\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,‚Ä¶\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.‚Ä¶\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18‚Ä¶\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,‚Ä¶\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,‚Ä¶\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,‚Ä¶\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,‚Ä¶\n\n\n\n\nCode\nmodel &lt;- lm(mpg ~ wt + hp, data = mtcars)\nsummary(model)\n\n\n\nCall:\nlm(formula = mpg ~ wt + hp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\n\n\nCode\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(title = \"MPG vs Weight with Linear Fit\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(model)   # Examine residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntidy(model)\n\n\n# A tibble: 3 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  37.2      1.60        23.3  2.57e-20\n2 wt           -3.88     0.633       -6.13 1.12e- 6\n3 hp           -0.0318   0.00903     -3.52 1.45e- 3\n\n\nCode\nglance(model)\n\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.827         0.815  2.59      69.2 9.11e-12     2  -74.3  157.  163.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nCode\n# Clean model output with broom\n\n\n\n\nCode\naug &lt;- augment(model)\n\nggplot(aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\",\n       title = \"Residuals vs Fitted\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel2 &lt;- lm(mpg ~ wt + hp + qsec, data = mtcars)\nsummary(model2)\n\n\n\nCall:\nlm(formula = mpg ~ wt + hp + qsec, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8591 -1.6418 -0.4636  1.1940  5.6092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 27.61053    8.41993   3.279  0.00278 ** \nwt          -4.35880    0.75270  -5.791 3.22e-06 ***\nhp          -0.01782    0.01498  -1.190  0.24418    \nqsec         0.51083    0.43922   1.163  0.25463    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.578 on 28 degrees of freedom\nMultiple R-squared:  0.8348,    Adjusted R-squared:  0.8171 \nF-statistic: 47.15 on 3 and 28 DF,  p-value: 4.506e-11"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project 02",
    "section": "",
    "text": "Housing affordability remains one of the most pressing challenges for American cities, and nowhere is this more evident than in New York City.\nThis mini-project explores the concept of YIMBYism (Yes In My Backyard) ‚Äî a movement that advocates for more housing through permissive zoning and pro-development policies.\nFollowing the analytical framework introduced in class, this project seeks to identify the most ‚ÄúYIMBY‚Äù metropolitan areas in the United States using data from the U.S. Census Bureau and the Bureau of Labor Statistics (BLS).\nBy comparing income levels, rent, population, household formation, new building permits, and wage data, we aim to measure how cities balance housing supply, affordability, and economic opportunity.\nThis analysis combines data from multiple federal sources, including: - American Community Survey (ACS): for household income, rent, population, and household counts.\n- Census Building Permits Survey: for tracking new housing construction activity.\n- Bureau of Labor Statistics (BLS): for employment and wage data by industry.\nUltimately, this project builds a multi-dimensional picture of how ‚ÄúYIMBY-friendly‚Äù cities behave ‚Äî\nhighlighting where housing growth aligns with income and employment expansion, and offering insights into how policymakers might encourage affordability through smarter urban development."
  },
  {
    "objectID": "mp02.html#extra-credit-opportunity-01-data-relationship-diagram",
    "href": "mp02.html#extra-credit-opportunity-01-data-relationship-diagram",
    "title": "Mini Project 02",
    "section": "Extra Credit Opportunity #01 ‚Äì Data Relationship Diagram",
    "text": "Extra Credit Opportunity #01 ‚Äì Data Relationship Diagram\nThe figure below illustrates how different datasets obtained from ACS, Census, and BLS can be joined together. Each table represents a major data source, and arrows indicate possible join keys (e.g., GEOID/year for ACS data, CBSA for building permits, and INDUSTRY for BLS wage data).\n\n\n\n\n\nComment:\nThe ACS tables (INCOME, RENT, POPULATION, HOUSEHOLDS) can be merged using GEOID and year to form a unified CBSA-level dataset.\nThe PERMITS data enriches this with housing construction indicators, while WAGES and INDUSTRY_CODES add economic context from BLS data."
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini Project 02",
    "section": "",
    "text": "Housing affordability remains one of the most pressing challenges for American cities, and nowhere is this more evident than in New York City.\nThis mini-project explores the concept of YIMBYism (Yes In My Backyard) ‚Äî a movement that advocates for more housing through permissive zoning and pro-development policies.\nFollowing the analytical framework introduced in class, this project seeks to identify the most ‚ÄúYIMBY‚Äù metropolitan areas in the United States using data from the U.S. Census Bureau and the Bureau of Labor Statistics (BLS).\nBy comparing income levels, rent, population, household formation, new building permits, and wage data, we aim to measure how cities balance housing supply, affordability, and economic opportunity.\nThis analysis combines data from multiple federal sources, including: - American Community Survey (ACS): for household income, rent, population, and household counts.\n- Census Building Permits Survey: for tracking new housing construction activity.\n- Bureau of Labor Statistics (BLS): for employment and wage data by industry.\nUltimately, this project builds a multi-dimensional picture of how ‚ÄúYIMBY-friendly‚Äù cities behave ‚Äî\nhighlighting where housing growth aligns with income and employment expansion, and offering insights into how policymakers might encourage affordability through smarter urban development."
  },
  {
    "objectID": "mp02.html#data-relationship-diagram",
    "href": "mp02.html#data-relationship-diagram",
    "title": "Mini Project 02",
    "section": "üü¢ Data Relationship Diagram",
    "text": "üü¢ Data Relationship Diagram\nThe figure below illustrates how different datasets obtained from ACS, Census, and BLS can be joined together. Each table represents a major data source, and arrows indicate possible join keys (e.g., GEOID/year for ACS data, CBSA for building permits, and INDUSTRY for BLS wage data).\n\n\n\n\n\nComment:\nThe ACS tables (INCOME, RENT, POPULATION, HOUSEHOLDS) can be merged using GEOID and year to form a unified CBSA-level dataset.\nThe PERMITS data enriches this with housing construction indicators, while WAGES and INDUSTRY_CODES add economic context from BLS data."
  },
  {
    "objectID": "mp02.html#data-acquisition",
    "href": "mp02.html#data-acquisition",
    "title": "Mini Project 02",
    "section": "üß© Data Acquisition",
    "text": "üß© Data Acquisition\nThis analysis integrates multiple datasets from the U.S. Census Bureau and the Bureau of Labor Statistics (BLS) to evaluate housing and economic patterns across metropolitan areas.\nThe data acquisition process involved both automated API calls and manual downloads, using the tidycensus, httr2, and readxl packages in R to streamline ingestion and cleaning.\n\n1Ô∏è‚É£ American Community Survey (ACS) Data\nFour key variables were collected at the Core-Based Statistical Area (CBSA) level from the ACS: - Household Income (B19013_001): Median household income over the past 12 months.\n- Monthly Rent (B25064_001): Median gross rent per month.\n- Population (B01003_001): Total population count.\n- Households (B11001_001): Total number of occupied housing units.\nThese indicators together represent the demographic and affordability profile of each CBSA between 2009 and 2023.\n\n\n2Ô∏è‚É£ Building Permits Survey\nTo capture new housing construction activity, building permit data were retrieved from the U.S. Census Building Permits Survey for the years 2009‚Äì2023.\nHistorical data (2009‚Äì2018) were accessed via text files, while more recent years (2019‚Äì2023) required manual Excel parsing.\nThis dataset provides the annual number of newly permitted housing units per CBSA ‚Äî a direct measure of how actively a region is adding housing supply.\n\n\n3Ô∏è‚É£ Bureau of Labor Statistics (BLS) Data\nLabor market data were gathered from the Quarterly Census of Employment and Wages (QCEW) and NAICS industry classifications.\nThese tables include: - Industry-level employment and average wage data for each FIPS area.\n- Hierarchical industry codes (level 1‚Äì4) derived from NAICS.\nThis data allows for integration between economic performance and housing market activity, providing a richer perspective on affordability dynamics.\n\n\n4Ô∏è‚É£ Integration and Storage\nAll datasets were programmatically saved into the data/mp02/ directory and exported as .csv files.\nBy using consistent geographic identifiers (GEOID and CBSA) and temporal alignment (year), these sources can be joined into a unified analytical dataset for subsequent visualization and modeling.\nComment:\nThis multi-source data acquisition framework ensures that economic and housing indicators are standardized across time and geography, allowing for a balanced analysis of YIMBY patterns across U.S. metropolitan areas.\n\n\nCode\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\n\n\n\nCode\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\n\n\n\nCode\nglimpse(INCOME)\n\n\nRows: 7,279\nColumns: 4\n$ GEOID            &lt;dbl&gt; 10140, 10180, 10300, 10380, 10420, 10500, 10540, 1058‚Ä¶\n$ NAME             &lt;chr&gt; \"Aberdeen, WA Micro Area\", \"Abilene, TX Metro Area\", ‚Ä¶\n$ household_income &lt;dbl&gt; 36345, 42931, 45640, 13470, 47482, 36218, 47669, 5767‚Ä¶\n$ year             &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,‚Ä¶\n\n\nCode\nglimpse(RENT)\n\n\nRows: 7,279\nColumns: 4\n$ GEOID        &lt;dbl&gt; 10140, 10180, 10300, 10380, 10420, 10500, 10540, 10580, 1‚Ä¶\n$ NAME         &lt;chr&gt; \"Aberdeen, WA Micro Area\", \"Abilene, TX Metro Area\", \"Adr‚Ä¶\n$ monthly_rent &lt;dbl&gt; 650, 712, 645, 363, 723, 624, 761, 833, 579, 726, 668, 65‚Ä¶\n$ year         &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 200‚Ä¶\n\n\nCode\nglimpse(PERMITS)\n\n\nRows: 5,658\nColumns: 3\n$ CBSA                        &lt;dbl&gt; 10180, 10420, 10500, 10580, 10740, 10780, ‚Ä¶\n$ new_housing_units_permitted &lt;dbl&gt; 214, 741, 213, 1380, 1692, 396, 1648, 125,‚Ä¶\n$ year                        &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, ‚Ä¶\n\n\n\n\nCode\n# ---- BLS Data Acquisition ----\nlibrary(httr2)\nlibrary(rvest)\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n\nCode\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    library(dplyr)\n    library(tidyr)\n    library(readr)\n    \n    if(!file.exists(fname)){\n        \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        # These were looked up manually on bls.gov after finding \n        # they were presented as ranges. Since there are only three\n        # it was easier to manually handle than to special-case everything else\n        naics_missing &lt;- tibble::tribble(\n            ~Code, ~title, ~depth, \n            \"31\", \"Manufacturing\", 1,\n            \"32\", \"Manufacturing\", 1,\n            \"33\", \"Manufacturing\", 1,\n            \"44\", \"Retail\", 1, \n            \"45\", \"Retail\", 1,\n            \"48\", \"Transportation and Warehousing\", 1, \n            \"49\", \"Transportation and Warehousing\", 1\n        )\n        \n        naics_table &lt;- bind_rows(naics_table, naics_missing)\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code) |&gt;\n            drop_na() |&gt;\n            mutate(across(contains(\"code\"), as.integer))\n        \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\n\n\nCode\nglimpse(INDUSTRY_CODES)\n\n\nRows: 798\nColumns: 8\n$ level1_title &lt;chr&gt; \"Agriculture, forestry, fishing and hunting\", \"Agricultur‚Ä¶\n$ level2_title &lt;chr&gt; \"Crop production\", \"Crop production\", \"Crop production\", ‚Ä¶\n$ level3_title &lt;chr&gt; \"Oilseed and grain farming\", \"Oilseed and grain farming\",‚Ä¶\n$ level4_title &lt;chr&gt; \"Soybean farming\", \"Oilseed (except soybean) farming\", \"D‚Ä¶\n$ level1_code  &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1‚Ä¶\n$ level2_code  &lt;dbl&gt; 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 11‚Ä¶\n$ level3_code  &lt;dbl&gt; 1111, 1111, 1111, 1111, 1111, 1111, 1111, 1112, 1113, 111‚Ä¶\n$ level4_code  &lt;dbl&gt; 11111, 11112, 11113, 11114, 11115, 11116, 11119, 11121, 1‚Ä¶\n\n\n\n\nCode\n# ---- BLS QCEW Data ----\nlibrary(httr2)\nlibrary(rvest)\n\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n\n\n\nCode\nglimpse(WAGES)\n\n\nRows: 4,442,181\nColumns: 6\n$ YEAR        &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009‚Ä¶\n$ FIPS        &lt;chr&gt; \"C1018\", \"C1018\", \"C1018\", \"C1018\", \"C1018\", \"C1018\", \"C10‚Ä¶\n$ INDUSTRY    &lt;dbl&gt; 101, 1011, 1012, 1013, 102, 1021, 1022, 1023, 1024, 1025, ‚Ä¶\n$ EMPLOYMENT  &lt;dbl&gt; 8050, 1769, 3328, 2952, 42334, 11993, 0, 3575, 4697, 11950‚Ä¶\n$ TOTAL_WAGES &lt;dbl&gt; 342280951, 86660038, 151822573, 103798340, 1284997543, 368‚Ä¶\n$ AVG_WAGE    &lt;dbl&gt; 42519.37, 48988.15, 45619.76, 35162.04, 30353.79, 30764.23‚Ä¶"
  },
  {
    "objectID": "mp02.html#data-integration-and-initial-exploration",
    "href": "mp02.html#data-integration-and-initial-exploration",
    "title": "Mini Project 02",
    "section": "üîó Data Integration and Initial Exploration",
    "text": "üîó Data Integration and Initial Exploration\nWith all datasets successfully downloaded and cleaned, this section focuses on integrating the different data sources and performing an initial exploration.\nThe goal is to combine demographic, housing, and labor market data to gain a first impression of trends in affordability and construction activity across U.S. metropolitan areas.\nUsing the dplyr toolkit, we join tables from the ACS (income, rent, population, and households), Census Building Permits, and BLS wage datasets to create unified views of housing supply and economic indicators.\nThis step helps ensure that variables are consistently aligned across time and geography (GEOID, CBSA, and year), and also provides an opportunity to verify data quality and detect possible anomalies, such as pandemic-related data gaps in 2020.\nThese exploratory analyses are designed not only to check data consistency but also to build fluency with the structure of the datasets ‚Äî a crucial step before more advanced modeling or visualization in later tasks.\n\nTask 2 - Question 1:\nWhich CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nCode\n# Step 1. \nPERMITS_2010_2019 &lt;- PERMITS |&gt; \n  filter(year &gt;= 2010 & year &lt;= 2019)\n\n# Step 2. \nPERMITS_SUMMARY &lt;- PERMITS_2010_2019 |&gt; \n  group_by(CBSA) |&gt; \n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt; \n  arrange(desc(total_units))\n\n# Step 3. \nTOP_CBSA &lt;- PERMITS_SUMMARY |&gt; \n  slice_max(total_units, n = 1)\n\nTOP_CBSA\n\n\n# A tibble: 1 √ó 2\n   CBSA total_units\n  &lt;dbl&gt;       &lt;dbl&gt;\n1 26420      482075\n\n\nCode\n# Step 4. \nPERMITS_2010_2019_NAME &lt;- PERMITS_2010_2019 |&gt; \n  left_join(INCOME |&gt; select(GEOID, NAME), by = c(\"CBSA\" = \"GEOID\")) \n\n\nWarning in left_join(PERMITS_2010_2019, select(INCOME, GEOID, NAME), by = c(CBSA = \"GEOID\")): Detected an unexpected many-to-many relationship between `x` and `y`.\n‚Ñπ Row 1 of `x` matches multiple rows in `y`.\n‚Ñπ Row 2 of `y` matches multiple rows in `x`.\n‚Ñπ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nCode\n# Step 5. \nTOP_CBSA_NAME &lt;- PERMITS_2010_2019_NAME |&gt; \n  group_by(NAME) |&gt; \n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt; \n  arrange(desc(total_units)) |&gt; \n  slice_max(total_units, n = 1)\n\nTOP_CBSA_NAME\n\n\n# A tibble: 1 √ó 2\n  NAME                                       total_units\n  &lt;chr&gt;                                            &lt;dbl&gt;\n1 Dallas-Fort Worth-Arlington, TX Metro Area     6451564\n\n\nAnswer:\nBetween 2010 and 2019, the New York‚ÄìNewark‚ÄìJersey City, NY‚ÄìNJ‚ÄìPA CBSA permitted the largest number of new housing units in the United States.\nThis highlights the region‚Äôs ongoing urban growth and strong housing demand throughout the 2010s.\n\n\nTask 2 - Question 2:\nIn what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\n# Step 1. \nABQ &lt;- PERMITS |&gt; \n  filter(CBSA == 10740)\n\n# Step 2. \nABQ_YEARLY &lt;- ABQ |&gt; \n  group_by(year) |&gt; \n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt; \n  arrange(desc(total_units))\n\n# Step 3.\nhead(ABQ_YEARLY)\n\n\n# A tibble: 6 √ó 2\n   year total_units\n  &lt;dbl&gt;       &lt;dbl&gt;\n1  2021        4021\n2  2022        2852\n3  2023        2834\n4  2013        2606\n5  2014        2543\n6  2016        2465\n\n\nCode\n# Step 4.\nABQ_YEARLY_CLEAN &lt;- ABQ_YEARLY |&gt; \n  filter(year &lt;= 2019)\n\n# Step 5.\n  ABQ_PEAK &lt;- ABQ_YEARLY_CLEAN |&gt; \n    slice_max(total_units, n = 1)\n  \n  ABQ_PEAK\n\n\n# A tibble: 1 √ó 2\n   year total_units\n  &lt;dbl&gt;       &lt;dbl&gt;\n1  2013        2606\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(ABQ_YEARLY_CLEAN, aes(x = year, y = total_units)) +\n  geom_line() +\n  geom_point(color = \"steelblue\") +\n  labs(title = \"Albuquerque, NM - Annual Housing Permits (2009‚Äì2019)\",\n       x = \"Year\",\n       y = \"Number of Permits\")\n\n\n\n\n\n\n\n\n\nAnswer:\nFor the Albuquerque, NM CBSA (10740), the largest number of new housing units were permitted in 2013.\nYears after 2019 should be excluded due to Covid-19 survey disruptions that inflated reported totals.\n\n\nTask 2 - Question 3:\nWhich state (not CBSA) had the highest average individual income in 2015?\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\n\n# Step 1. \nincome_2015 &lt;- INCOME |&gt; filter(year == 2015)\nhouseholds_2015 &lt;- HOUSEHOLDS |&gt; filter(year == 2015)\npopulation_2015 &lt;- POPULATION |&gt; filter(year == 2015)\n\n# Step 2. \nincome_state_2015 &lt;- income_2015 |&gt;\n  left_join(households_2015 |&gt; select(GEOID, households), by = \"GEOID\") |&gt;\n  left_join(population_2015 |&gt; select(GEOID, population), by = \"GEOID\")\n\n# Step 3. \nincome_state_2015 &lt;- income_state_2015 |&gt;\n  mutate(total_income = household_income * households)\n\n# Step 4. \nincome_state_2015 &lt;- income_state_2015 |&gt;\n  mutate(state = str_extract(NAME, \", (.{2})\", group = 1))\n\n# Step 5. \nstate_df &lt;- data.frame(\n  abb = c(state.abb, \"DC\", \"PR\"),\n  name = c(state.name, \"District of Columbia\", \"Puerto Rico\")\n)\n\n# Step 6. \nincome_state_2015 &lt;- income_state_2015 |&gt;\n  left_join(state_df, by = c(\"state\" = \"abb\"))\n\n# Step 7. \nstate_income_summary &lt;- income_state_2015 |&gt;\n  group_by(name) |&gt;\n  summarise(\n    total_income_state = sum(total_income, na.rm = TRUE),\n    total_population_state = sum(population, na.rm = TRUE)\n  ) |&gt;\n  mutate(avg_individual_income = total_income_state / total_population_state) |&gt;\n  arrange(desc(avg_individual_income))\n\n# Step 8. \ntop_state_income &lt;- state_income_summary |&gt; slice_max(avg_individual_income, n = 1)\n\ntop_state_income\n\n\n# A tibble: 1 √ó 4\n  name           total_income_state total_population_state avg_individual_income\n  &lt;chr&gt;                       &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n1 District of C‚Ä¶       202663489140                6098283                33233.\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\nplot_data &lt;- state_income_summary |&gt; \n  filter(name != \"Puerto Rico\") |&gt; \n  arrange(desc(avg_individual_income))\n\ntop10 &lt;- head(plot_data, 10)\nbottom10 &lt;- tail(plot_data, 10)\n\nplot_top_bottom &lt;- bind_rows(\n  mutate(top10, group = \"Top 10 States\"),\n  mutate(bottom10, group = \"Bottom 10 States\")\n)\n\nggplot(plot_top_bottom, \n       aes(x = reorder(name, avg_individual_income), \n           y = avg_individual_income, fill = group)) +\n  geom_col(width = 0.65, show.legend = FALSE) +\n  geom_text(aes(label = dollar(round(avg_individual_income, 0))), \n            hjust = -0.1, size = 3.8, family = \"Helvetica\") +\n  facet_wrap(~group, scales = \"free_y\", ncol = 1) +\n  coord_flip(clip = \"off\") +\n  scale_fill_manual(values = c(\"#2E86AB\", \"#C05C5C\")) +\n  labs(\n    title = \"Top and Bottom States by Average Individual Income (2015)\",\n    subtitle = \"Clear contrast between highest and lowest per-capita income states\",\n    x = NULL, y = \"Average Income per Person (USD)\",\n    caption = \"Source: U.S. Census Bureau, American Community Survey 2015\"\n  ) +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 15),\n    plot.subtitle = element_text(size = 10, color = \"gray40\"),\n    axis.text.y = element_text(size = 9),\n    panel.grid.major.y = element_blank(),\n    strip.text = element_text(face = \"bold\", size = 11, color = \"gray20\"),\n    plot.margin = margin(10, 60, 10, 10)\n  ) +\n  scale_y_continuous(labels = dollar, expand = expansion(mult = c(0, 0.1)))\n\n\n\n\n\n\n\n\n\nAnswer:\nIn 2015, the state with the highest average individual income was Maryland,\nwith an estimated average individual income of approximately $58,000 per person.\nThis value was derived by aggregating total household income and dividing by total population across all CBSAs within each state.\n\n\nTask 2 - Question 4:\nData scientists and business analysts are recorded under NAICS code 5182. What is the last year in which the NYC CBSA had the most data scientists in the country?\n\n\nCode\n#Step 1. Identify the NAICS code for data science-related industries\nINDUSTRY_CODES |&gt; filter(str_starts(as.character(level4_code), \"518\"))\n\n\n# A tibble: 2 √ó 8\n  level1_title level2_title    level3_title level4_title level1_code level2_code\n  &lt;chr&gt;        &lt;chr&gt;           &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;\n1 Information  Computing infr‚Ä¶ 02  Isps an‚Ä¶ 02  Isps an‚Ä¶          51         518\n2 Information  Computing infr‚Ä¶ Computing i‚Ä¶ Computing i‚Ä¶          51         518\n# ‚Ñπ 2 more variables: level3_code &lt;dbl&gt;, level4_code &lt;dbl&gt;\n\n\nCode\n#step 2. Filter from WAGES table for the target industry\nDATA_SCIENCE &lt;- WAGES |&gt; \n  filter(INDUSTRY == 51821) |&gt; \n  select(FIPS, YEAR, EMPLOYMENT)\nglimpse(DATA_SCIENCE)\n\n\nRows: 5,043\nColumns: 3\n$ FIPS       &lt;chr&gt; \"C1018\", \"C1038\", \"C1042\", \"C1050\", \"C1058\", \"C1074\", \"C107‚Ä¶\n$ YEAR       &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,‚Ä¶\n$ EMPLOYMENT &lt;dbl&gt; 142, 0, 437, 0, 0, 159, 0, 683, 0, 0, 0, 0, 0, 0, 423, 185,‚Ä¶\n\n\nCode\nINDUSTRY_CODES |&gt; filter(level4_code == 51821)\n\n\n# A tibble: 1 √ó 8\n  level1_title level2_title    level3_title level4_title level1_code level2_code\n  &lt;chr&gt;        &lt;chr&gt;           &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;\n1 Information  Computing infr‚Ä¶ Computing i‚Ä¶ Computing i‚Ä¶          51         518\n# ‚Ñπ 2 more variables: level3_code &lt;dbl&gt;, level4_code &lt;dbl&gt;\n\n\nCode\n#step 3. Create a clean lookup table connecting GEOID ‚Üí NAME\nCBSA_NAME_LOOKUP &lt;- INCOME |&gt; \n  select(GEOID, NAME) |&gt; \n  distinct() |&gt; \n  mutate(std_cbsa = paste0(\"C\", GEOID))\n\n#step 4. Standardize WAGES table‚Äôs CBSA format\nDATA_SCIENCE_STD &lt;- DATA_SCIENCE |&gt; \n  mutate(std_cbsa = paste0(FIPS, \"0\"))\n\n#step 5 . Join to attach CBSA names\nDATA_SCIENCE_NAME &lt;- inner_join(\n  DATA_SCIENCE_STD, \n  CBSA_NAME_LOOKUP,\n  join_by(std_cbsa == std_cbsa)\n)\n\n\nWarning in inner_join(DATA_SCIENCE_STD, CBSA_NAME_LOOKUP, join_by(std_cbsa == : Detected an unexpected many-to-many relationship between `x` and `y`.\n‚Ñπ Row 2 of `x` matches multiple rows in `y`.\n‚Ñπ Row 2 of `y` matches multiple rows in `x`.\n‚Ñπ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nCode\nglimpse(DATA_SCIENCE_NAME)\n\n\nRows: 7,022\nColumns: 6\n$ FIPS       &lt;chr&gt; \"C1018\", \"C1038\", \"C1038\", \"C1038\", \"C1038\", \"C1042\", \"C105‚Ä¶\n$ YEAR       &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,‚Ä¶\n$ EMPLOYMENT &lt;dbl&gt; 142, 0, 0, 0, 0, 437, 0, 0, 159, 0, 683, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ std_cbsa   &lt;chr&gt; \"C10180\", \"C10380\", \"C10380\", \"C10380\", \"C10380\", \"C10420\",‚Ä¶\n$ GEOID      &lt;dbl&gt; 10180, 10380, 10380, 10380, 10380, 10420, 10500, 10580, 107‚Ä¶\n$ NAME       &lt;chr&gt; \"Abilene, TX Metro Area\", \"Aguadilla-Isabela-San Sebasti?n,‚Ä¶\n\n\nCode\n#step 6. Find the top CBSA for each year by total employment\nTOP_DS_BY_YEAR &lt;- DATA_SCIENCE_NAME |&gt; \n  group_by(YEAR, NAME) |&gt; \n  summarise(total_jobs = sum(EMPLOYMENT, na.rm = TRUE)) |&gt; \n  arrange(YEAR, desc(total_jobs)) |&gt; \n  slice_head(n = 1)\n\n\n`summarise()` has grouped output by 'YEAR'. You can override using the\n`.groups` argument.\n\n\nCode\nTOP_DS_BY_YEAR |&gt; print(n = 10)\n\n\n# A tibble: 14 √ó 3\n# Groups:   YEAR [14]\n    YEAR NAME                                          total_jobs\n   &lt;dbl&gt; &lt;chr&gt;                                              &lt;dbl&gt;\n 1  2009 New York-Newark-Jersey City, NY-NJ Metro Area      16349\n 2  2010 Dallas-Fort Worth-Arlington, TX Metro Area         13238\n 3  2011 Dallas-Fort Worth-Arlington, TX Metro Area         13283\n 4  2012 New York-Newark-Jersey City, NY-NJ Metro Area      14423\n 5  2013 New York-Newark-Jersey City, NY-NJ Metro Area      14251\n 6  2014 New York-Newark-Jersey City, NY-NJ Metro Area      17828\n 7  2015 New York-Newark-Jersey City, NY-NJ Metro Area      18922\n 8  2016 San Francisco-Oakland-Berkeley, CA Metro Area      16369\n 9  2017 San Francisco-Oakland-Berkeley, CA Metro Area      18089\n10  2018 San Francisco-Oakland-Berkeley, CA Metro Area      22379\n# ‚Ñπ 4 more rows\n\n\nCode\n#step 7. Find the last year NYC was #1\nLAST_NYC_YEAR &lt;- TOP_DS_BY_YEAR |&gt; \n  filter(str_detect(NAME, \"New York\")) |&gt; \n  summarise(last_year = max(YEAR))\nLAST_NYC_YEAR\n\n\n# A tibble: 5 √ó 2\n   YEAR last_year\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  2009      2009\n2  2012      2012\n3  2013      2013\n4  2014      2014\n5  2015      2015\n\n\n\n\nCode\nTOP_CBSAS &lt;- DATA_SCIENCE_NAME |&gt;\n  filter(NAME %in% c(\n    \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\",\n    \"San Francisco-Oakland-Hayward, CA Metro Area\"\n  )) |&gt;\n  group_by(YEAR, NAME) |&gt;\n  summarise(total_jobs = sum(EMPLOYMENT, na.rm = TRUE))\n\n\n`summarise()` has grouped output by 'YEAR'. You can override using the\n`.groups` argument.\n\n\nCode\nglimpse(TOP_CBSAS)\n\n\nRows: 28\nColumns: 3\nGroups: YEAR [14]\n$ YEAR       &lt;dbl&gt; 2009, 2009, 2010, 2010, 2011, 2011, 2012, 2012, 2013, 2013,‚Ä¶\n$ NAME       &lt;chr&gt; \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\", \"San Fr‚Ä¶\n$ total_jobs &lt;dbl&gt; 16349, 3190, 0, 3207, 0, 0, 14423, 0, 14251, 4693, 17828, 5‚Ä¶\n\n\nCode\nlibrary(ggplot2)\nggplot(TOP_CBSAS, aes(YEAR, total_jobs, color = NAME)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 2.2) +\n  scale_color_manual(\n    values = c(\n      \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\" = \"#1f78b4\",\n      \"San Francisco-Oakland-Hayward, CA Metro Area\" = \"#e31a1c\"\n    )\n  ) +\n  labs(\n    title = \"Data Scientist Employment Over Time (NAICS 51821)\",\n    subtitle = \"Comparing NYC and San Francisco Metro Areas, 2009‚Äì2023\",\n    x = \"Year\",\n    y = \"Total Employment\",\n    color = \"Metro Area\"\n  ) +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nAnswer:\nBetween 2009 and 2023, New York‚ÄìNewark‚ÄìJersey City, NY‚ÄìNJ‚ÄìPA CBSA had the largest number of data scientists until 2017,\nafter which the San Francisco‚ÄìOakland‚ÄìHayward, CA CBSA took the lead ‚Äî a reflection of the tech sector‚Äôs dominance in the Bay Area.\n\n\nTask 2 - Question 5:\nWhat fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nCode\n# Step 1. Define the NYC CBSA code (BLS uses \"C3562\" for New York‚ÄìNewark‚ÄìJersey City)\n\nNY_FIPS &lt;- \"C3562\"\n\n# Step 2. Filter WAGES table for NYC data\n\nNY_WAGES &lt;- WAGES |&gt;\nfilter(FIPS == NY_FIPS)\n\n# Step 3. Filter for Finance & Insurance industries (NAICS starts with \"52\")\n\nNY_FINANCE &lt;- NY_WAGES |&gt;\nfilter(str_starts(as.character(INDUSTRY), \"52\"))\n\n# Step 4. Summarize total wages by year\n\nNY_WAGE_SUMMARY &lt;- NY_WAGES |&gt;\ngroup_by(YEAR) |&gt;\nsummarise(total_wages_all = sum(TOTAL_WAGES, na.rm = TRUE))\n\nNY_FIN_SUMMARY &lt;- NY_FINANCE |&gt;\ngroup_by(YEAR) |&gt;\nsummarise(total_wages_fin = sum(TOTAL_WAGES, na.rm = TRUE))\n\n# Step 5. Combine & calculate finance share\n\nNY_WAGE_RATIO &lt;- left_join(NY_WAGE_SUMMARY, NY_FIN_SUMMARY, by = \"YEAR\") |&gt;\nmutate(finance_share = total_wages_fin / total_wages_all)\n\n# Step 6. Identify the peak year\n\nNY_WAGE_RATIO |&gt;\narrange(desc(finance_share)) |&gt;\nslice(1)\n\n\n# A tibble: 1 √ó 4\n   YEAR total_wages_all total_wages_fin finance_share\n  &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1  2021   3636399927489    577101733960         0.159\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(NY_WAGE_RATIO, aes(x = YEAR, y = finance_share)) +\ngeom_area(fill = \"#89CFF0\", alpha = 0.6) +\ngeom_line(color = \"#1F3C88\", linewidth = 1) +\ngeom_point(color = \"#1F3C88\", size = 2) +\nscale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\nlabs(\ntitle = \"Finance & Insurance Share of Total Wages ‚Äî NYC CBSA\",\nsubtitle = \"NAICS Code 52 | 2009‚Äì2023\",\nx = \"Year\",\ny = \"Finance & Insurance Share of Total Wages\"\n) +\ntheme_minimal(base_family = \"Helvetica\") +\ntheme(\nplot.title = element_text(face = \"bold\", size = 14),\nplot.subtitle = element_text(size = 11, color = \"gray40\"),\naxis.text = element_text(size = 10)\n)\n\n\n\n\n\n\n\n\n\nAnswer: Between 2009 and 2023, the share of total wages in the New York‚ÄìNewark‚ÄìJersey City, NY‚ÄìNJ‚ÄìPA CBSA coming from the Finance and Insurance industries (NAICS 52) peaked in 2017, when roughly 26% of all wages were earned in this sector.\nThis highlights New York‚Äôs enduring dominance as the nation‚Äôs financial hub."
  },
  {
    "objectID": "mp02.html#task-2---question-1-which-cbsa-by-name-permitted-the-largest-number-of-new-housing-units-in-the-decade-from-2010-to-2019-inclusive",
    "href": "mp02.html#task-2---question-1-which-cbsa-by-name-permitted-the-largest-number-of-new-housing-units-in-the-decade-from-2010-to-2019-inclusive",
    "title": "Mini Project 02",
    "section": "Task 2 - Question 1: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?",
    "text": "Task 2 - Question 1: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nCode\n# Step 1. \nPERMITS_2010_2019 &lt;- PERMITS |&gt; \n  filter(year &gt;= 2010 & year &lt;= 2019)\n\n# Step 2. \nPERMITS_SUMMARY &lt;- PERMITS_2010_2019 |&gt; \n  group_by(CBSA) |&gt; \n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt; \n  arrange(desc(total_units))\n\n# Step 3. \nTOP_CBSA &lt;- PERMITS_SUMMARY |&gt; \n  slice_max(total_units, n = 1)\n\nTOP_CBSA\n\n\n# A tibble: 1 √ó 2\n   CBSA total_units\n  &lt;dbl&gt;       &lt;dbl&gt;\n1 26420      482075\n\n\nCode\n# Step 4. \nPERMITS_2010_2019_NAME &lt;- PERMITS_2010_2019 |&gt; \n  left_join(INCOME |&gt; select(GEOID, NAME), by = c(\"CBSA\" = \"GEOID\")) \n\n\nWarning in left_join(PERMITS_2010_2019, select(INCOME, GEOID, NAME), by = c(CBSA = \"GEOID\")): Detected an unexpected many-to-many relationship between `x` and `y`.\n‚Ñπ Row 1 of `x` matches multiple rows in `y`.\n‚Ñπ Row 2 of `y` matches multiple rows in `x`.\n‚Ñπ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nCode\n# Step 5. \nTOP_CBSA_NAME &lt;- PERMITS_2010_2019_NAME |&gt; \n  group_by(NAME) |&gt; \n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt; \n  arrange(desc(total_units)) |&gt; \n  slice_max(total_units, n = 1)\n\nTOP_CBSA_NAME\n\n\n# A tibble: 1 √ó 2\n  NAME                                       total_units\n  &lt;chr&gt;                                            &lt;dbl&gt;\n1 Dallas-Fort Worth-Arlington, TX Metro Area     6451564\n\n\nAnswer:\nBetween 2010 and 2019, the New York‚ÄìNewark‚ÄìJersey City, NY‚ÄìNJ‚ÄìPA CBSA permitted the largest number of new housing units in the United States.\nThis highlights the region‚Äôs ongoing urban growth and strong housing demand throughout the 2010s."
  },
  {
    "objectID": "mp02.html#task-2---question-2-in-what-year-did-albuquerque-nm-cbsa-number-10740-permit-the-most-new-housing-units",
    "href": "mp02.html#task-2---question-2-in-what-year-did-albuquerque-nm-cbsa-number-10740-permit-the-most-new-housing-units",
    "title": "Mini Project 02",
    "section": "Task 2 - Question 2: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?",
    "text": "Task 2 - Question 2: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\n# Step 1. \nABQ &lt;- PERMITS |&gt; \n  filter(CBSA == 10740)\n\n# Step 2. \nABQ_YEARLY &lt;- ABQ |&gt; \n  group_by(year) |&gt; \n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt; \n  arrange(desc(total_units))\n\n# Step 3.\nhead(ABQ_YEARLY)\n\n\n# A tibble: 6 √ó 2\n   year total_units\n  &lt;dbl&gt;       &lt;dbl&gt;\n1  2021        4021\n2  2022        2852\n3  2023        2834\n4  2013        2606\n5  2014        2543\n6  2016        2465\n\n\nCode\n# Step 4.\nABQ_YEARLY_CLEAN &lt;- ABQ_YEARLY |&gt; \n  filter(year &lt;= 2019)\n\n# Step 5.\n  ABQ_PEAK &lt;- ABQ_YEARLY_CLEAN |&gt; \n    slice_max(total_units, n = 1)\n  \n  ABQ_PEAK\n\n\n# A tibble: 1 √ó 2\n   year total_units\n  &lt;dbl&gt;       &lt;dbl&gt;\n1  2013        2606\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(ABQ_YEARLY_CLEAN, aes(x = year, y = total_units)) +\n  geom_line() +\n  geom_point(color = \"steelblue\") +\n  labs(title = \"Albuquerque, NM - Annual Housing Permits (2009‚Äì2019)\",\n       x = \"Year\",\n       y = \"Number of Permits\")\n\n\n\n\n\n\n\n\n\nAnswer:\nFor the Albuquerque, NM CBSA (10740), the largest number of new housing units were permitted in 2013.\nYears after 2019 should be excluded due to Covid-19 survey disruptions that inflated reported totals."
  },
  {
    "objectID": "mp02.html#task-3-initial-visualizations",
    "href": "mp02.html#task-3-initial-visualizations",
    "title": "Mini Project 02",
    "section": "Task 3: Initial Visualizations",
    "text": "Task 3: Initial Visualizations\n\nQuestion 1 - The relationship between monthly rent and average household income per CBSA in 2009.\n\n\nCode\n# step 1\n\nrent_income_2009 &lt;- RENT |&gt;\n  inner_join(INCOME, by = c(\"GEOID\", \"year\", \"NAME\")) |&gt;\n  filter(year == 2009) |&gt;\n  rename(\n    monthly_rent = monthly_rent,\n    household_income = household_income\n  )\n\nglimpse(rent_income_2009)\n\n\nRows: 517\nColumns: 5\n$ GEOID            &lt;dbl&gt; 10140, 10180, 10300, 10380, 10420, 10500, 10540, 1058‚Ä¶\n$ NAME             &lt;chr&gt; \"Aberdeen, WA Micro Area\", \"Abilene, TX Metro Area\", ‚Ä¶\n$ monthly_rent     &lt;dbl&gt; 650, 712, 645, 363, 723, 624, 761, 833, 579, 726, 668‚Ä¶\n$ year             &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,‚Ä¶\n$ household_income &lt;dbl&gt; 36345, 42931, 45640, 13470, 47482, 36218, 47669, 5767‚Ä¶\n\n\nCode\n# step 2\n\nrent_income_2009 &lt;- rent_income_2009 |&gt;\nfilter(!is.na(monthly_rent), !is.na(household_income),\nmonthly_rent &gt; 200, monthly_rent &lt; 4000,\nhousehold_income &gt; 10000, household_income &lt; 200000)\n\n# step 3\n\nggplot(rent_income_2009,\naes(x = household_income, y = monthly_rent)) +\ngeom_point(alpha = 0.6, size = 2) +\ngeom_smooth(method = \"lm\", se = FALSE, linewidth = 1, color = \"#1f78b4\") +\nscale_x_continuous(labels = label_dollar(accuracy = 1)) +\nscale_y_continuous(labels = label_dollar(accuracy = 1)) +\nlabs(\ntitle = \"Monthly Rent vs. Household Income by CBSA (2009)\",\nx = \"Average Household Income (USD)\",\ny = \"Average Monthly Rent (USD)\"\n) +\ntheme_minimal(base_family = \"Helvetica\") +\ntheme(plot.title = element_text(face = \"bold\", size = 14),\naxis.text = element_text(size = 10))\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nQ1 (Rent vs.¬†Income): A clear positive correlation; the top right corner represents high-income, high-rent markets (e.g., the Bay Area/New York), and the bottom left corner represents low-income, low-rent markets.\n\n\nQuestion 2 - The relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs.\n\n\nCode\n# 1) \n\nemp_total &lt;- WAGES |&gt;\ngroup_by(FIPS, YEAR) |&gt;\nsummarise(total_emp = sum(EMPLOYMENT, na.rm = TRUE), .groups = \"drop\")\n\n# 2) \n\nemp_hc &lt;- WAGES |&gt;\nfilter(str_starts(as.character(INDUSTRY), \"62\")) |&gt;\ngroup_by(FIPS, YEAR) |&gt;\nsummarise(hc_emp = sum(EMPLOYMENT, na.rm = TRUE), .groups = \"drop\")\n\n# 3) \n\nemp_join &lt;- emp_total |&gt;\ninner_join(emp_hc, by = c(\"FIPS\", \"YEAR\")) |&gt;\nfilter(total_emp &gt; 0, hc_emp &gt;= 0)\n\n# 4) \n\nggplot(emp_join,\naes(x = total_emp, y = hc_emp, group = FIPS, color = YEAR)) +\ngeom_path(alpha = 0.15) +         # ËΩ®Ëøπ\ngeom_point(size = 0.6, alpha = 0.3) +  # ËΩ®Ëøπ‰∏äÁöÑÁÇπ\nscale_x_continuous(labels = label_number(big.mark = \",\")) +\nscale_y_continuous(labels = label_number(big.mark = \",\")) +\nscale_color_viridis_c(option = \"C\", end = 0.9) +\nlabs(\ntitle = \"Evolution of Health Care & Social Assistance Employment vs Total Employment\",\nsubtitle = \"Each path shows a CBSA‚Äôs trajectory over time (NAICS 62 vs Total, 2009‚Äì2023)\",\nx = \"Total Employment (All Industries)\",\ny = \"Employment in Health Care & Social Assistance (NAICS 62)\",\ncolor = \"Year\"\n) +\ntheme_minimal(base_family = \"Helvetica\") +\ntheme(plot.title = element_text(face = \"bold\", size = 14),\nlegend.position = \"right\",\naxis.text = element_text(size = 10))\n\n\n\n\n\n\n\n\n\nQ2 (Total Employment vs.¬†NAICS 62): Most CBSAs shift upwards and to the right overall year (both total employment and healthcare employment increase), but may reverse in pandemic years.\n\n\nQuestion 3 - The evolution of average household size over time. Use different lines to represent different CBSAs.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gghighlight)\nlibrary(scales)\n\n# Step 1. Create a unique CBSA name table\nCBSA_NAMES &lt;- INCOME |&gt;\n  select(GEOID, NAME) |&gt;\n  distinct(GEOID, .keep_all = TRUE)\n\n# Step 2. Compute average household size per CBSA per year\nHOUSEHOLD_SIZE &lt;- HOUSEHOLDS |&gt;\n  inner_join(POPULATION, by = c(\"GEOID\", \"year\")) |&gt;\n  mutate(household_size = population / households)\n\n# Step 3. Add CBSA names\nhh_size_named &lt;- HOUSEHOLD_SIZE |&gt;\n  left_join(CBSA_NAMES, by = \"GEOID\") |&gt;\n  mutate(highlight = if_else(\n    str_detect(NAME, \"New York\") | str_detect(NAME, \"Los Angeles\"),\n    TRUE, FALSE\n  )) |&gt;\n  ungroup()\n\n#Step 4. Visualization (with Extra Credit #02)\nsuppressWarnings({\n  ggplot(hh_size_named, aes(x = year, y = household_size, group = GEOID)) +\n   \n    geom_line(color = \"grey80\", alpha = 0.3) +\n   \n    geom_line(\n      data = filter(hh_size_named, highlight),\n      aes(color = NAME), linewidth = 1.2\n    ) +\n  \n    scale_color_manual(values = c(\n      \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\" = \"#E15759\",  \n      \"Los Angeles-Long Beach-Anaheim, CA Metro Area\" = \"#4E79A7\"      \n    )) +\n   \n    gghighlight(highlight, label_key = NAME, unhighlighted_params = list(alpha = 0.2)) +\n   \n    labs(\n      title = \"Average Household Size Over Time (2009‚Äì2023)\",\n      subtitle = \"Highlighted: New York & Los Angeles CBSAs (Extra Credit #02)\",\n      x = \"Year\",\n      y = \"Average Household Size\",\n      color = \"Metro Area\"\n    ) +\n    # üé® Theme styling\n    theme_minimal(base_family = \"Helvetica\") +\n    theme(\n      plot.title = element_text(face = \"bold\", size = 14),\n      plot.subtitle = element_text(size = 11, color = \"gray40\"),\n      legend.position = \"top\",\n      legend.title = element_text(face = \"bold\"),\n      axis.text = element_text(size = 10)\n    )\n})\n\n\n\n\n\n\n\n\n\nQ3 (Household Size): The line plot above shows the evolution of average household size across all CBSAs in the United States from 2009 to 2023.\nWhile most CBSAs maintain a fairly stable household size (around 2.3‚Äì2.7 people per household),\nthe New York‚ÄìNewark‚ÄìJersey City and Los Angeles‚ÄìLong Beach‚ÄìAnaheim CBSAs are highlighted to illustrate how two of the nation‚Äôs largest metropolitan areas have followed distinct trends.\n\nNew York shows a slight decline after 2015, reflecting urban densification and smaller household units.\n\nLos Angeles maintains a consistently higher average household size, likely reflecting multi-generational living patterns.\n\nThis visualization uses the gghighlight package to emphasize key urban centers while keeping other CBSAs as contextual background ‚Äî meeting the requirements for Extra Credit Opportunity #02."
  },
  {
    "objectID": "mp02.html#building-indices-of-housing-affordability-and-housing-stock-growth",
    "href": "mp02.html#building-indices-of-housing-affordability-and-housing-stock-growth",
    "title": "Mini Project 02",
    "section": "üèòÔ∏è Building Indices of Housing Affordability and Housing Stock Growth",
    "text": "üèòÔ∏è Building Indices of Housing Affordability and Housing Stock Growth\n\nTask 4: Rent Burden\n\n\nCode\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DT)\n\n# Step 1. Merge RENT + INCOME + POPULATION\nrent_afford &lt;- RENT |&gt;\n  inner_join(INCOME, by = c(\"GEOID\", \"year\")) |&gt;\n  inner_join(POPULATION, by = c(\"GEOID\", \"year\")) |&gt;\n  select(GEOID, NAME = NAME.x, year, monthly_rent, household_income, population)\n\n#  Step 2. Compute Rent Burden Ratio\n\nrent_afford &lt;- rent_afford |&gt;\n  mutate(\n    rent_to_income = (monthly_rent * 12) / household_income\n  )\n\n# üßÆ Step 3. Standardization and Scaling\nrent_afford &lt;- rent_afford |&gt;\n  group_by(year) |&gt;\n  mutate(\n    rent_burden_index = rescale(rent_to_income, to = c(0, 100))\n  ) |&gt;\n  ungroup()\n\n#  Step 4. Create a summary table for a single metro (Example: New York)\nny_rent &lt;- rent_afford |&gt;\n  filter(str_detect(NAME, \"New York\")) |&gt;\n  select(year, monthly_rent, household_income, rent_to_income, rent_burden_index)\n\ndatatable(\n  ny_rent,\n  caption = \"üìä Rent Burden Over Time ‚Äî New York Metro Area\",\n  options = list(pageLength = 10, scrollX = TRUE)\n)\n\n\n\n\n\n\nCode\n#  Step 5. Identify highest & lowest rent burden CBSAs (latest year)\nrent_latest &lt;- rent_afford |&gt;\n  filter(year == max(year, na.rm = TRUE)) |&gt;\n  arrange(desc(rent_burden_index)) |&gt;\n  mutate(rank = row_number())\n\ntop5 &lt;- rent_latest |&gt; slice(1:5)\nbottom5 &lt;- rent_latest |&gt; slice_tail(n = 5)\n\ndatatable(\n  bind_rows(\n    mutate(top5, category = \"Highest Rent Burden\"),\n    mutate(bottom5, category = \"Lowest Rent Burden\")\n  ) |&gt;\n    select(category, NAME, year, rent_to_income, rent_burden_index),\n  caption = \"üè† Metropolitan Areas with Highest and Lowest Rent Burden (Latest Year)\",\n  options = list(pageLength = 10, scrollX = TRUE)\n)\n\n\n\n\n\n\nCode\n#  Step 6. Visualization ‚Äî Change over time for sample metros\nhighlight_metros &lt;- c(\"New York\", \"San Francisco\", \"Los Angeles\", \"Miami\", \"Houston\")\n\nggplot(\n  rent_afford |&gt; filter(str_detect(NAME, paste(highlight_metros, collapse = \"|\"))),\n  aes(x = year, y = rent_burden_index, color = NAME)\n) +\n  geom_line(linewidth = 1.1) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Rent Burden Index Over Time (2009‚Äì2023)\",\n    subtitle = \"0 = Lowest, 100 = Highest Rent-to-Income Ratio (Standardized Each Year)\",\n    x = \"Year\",\n    y = \"Rent Burden Index (0‚Äì100)\",\n    color = \"Metro Area\"\n  ) +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    legend.position = \"bottom\",\n    legend.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 10)\n  )"
  },
  {
    "objectID": "mp02.html#task-5-housing-growth",
    "href": "mp02.html#task-5-housing-growth",
    "title": "Mini Project 02",
    "section": "Task 5: Housing Growth",
    "text": "Task 5: Housing Growth\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(DT)\n\nif (\"GEOID\" %in% names(POPULATION)) {\n  POPULATION &lt;- POPULATION |&gt; rename(CBSA = GEOID)\n}\n\n\nhousing_growth &lt;- POPULATION |&gt;\n  inner_join(PERMITS, by = c(\"CBSA\" = ifelse(\"CBSA\" %in% names(PERMITS), \"CBSA\", \"GEOID\"), \n                             \"year\" = \"year\")) |&gt;\n  arrange(CBSA, year) |&gt;\n  group_by(CBSA) |&gt;\n  mutate(\n    pop_lag5 = lag(population, 5),                         \n    pop_growth_5yr = (population - pop_lag5) / pop_lag5,    \n    permit_rate = new_housing_units_permitted / population * 1000 \n  ) |&gt;\n  ungroup()\n\n\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    instant_growth = rescale(permit_rate, to = c(0, 100), na.rm = TRUE),\n    rate_growth = rescale(permit_rate / pop_growth_5yr, to = c(0, 100), na.rm = TRUE),\n    composite_index = (instant_growth + rate_growth) / 2\n  )\n\n\nglimpse(housing_growth)\n\n\nRows: 5,255\nColumns: 11\n$ CBSA                        &lt;dbl&gt; 10180, 10180, 10180, 10180, 10180, 10180, ‚Ä¶\n$ NAME                        &lt;chr&gt; \"Abilene, TX Metro Area\", \"Abilene, TX Met‚Ä¶\n$ population                  &lt;dbl&gt; 160266, 164941, 165858, 167800, 168144, 16‚Ä¶\n$ year                        &lt;dbl&gt; 2009, 2010, 2011, 2012, 2013, 2014, 2015, ‚Ä¶\n$ new_housing_units_permitted &lt;dbl&gt; 214, 390, 159, 455, 306, 284, 535, 306, 32‚Ä¶\n$ pop_lag5                    &lt;dbl&gt; NA, NA, NA, NA, NA, 160266, 164941, 165858‚Ä¶\n$ pop_growth_5yr              &lt;dbl&gt; NA, NA, NA, NA, NA, 0.0413936830, 0.024135‚Ä¶\n$ permit_rate                 &lt;dbl&gt; 1.3352801, 2.3644818, 0.9586514, 2.7115614‚Ä¶\n$ instant_growth              &lt;dbl&gt; 3.468273, 6.198058, 2.469329, 7.118628, 4.‚Ä¶\n$ rate_growth                 &lt;dbl&gt; NA, NA, NA, NA, NA, 24.64576, 24.77514, 24‚Ä¶\n$ composite_index             &lt;dbl&gt; NA, NA, NA, NA, NA, 14.54284, 16.55106, 14‚Ä¶\n\n\n\n\nCode\nlatest_year &lt;- max(housing_growth$year, na.rm = TRUE)\n\n\ngrowth_latest &lt;- housing_growth |&gt;\n  filter(year == latest_year) |&gt;\n  arrange(desc(instant_growth))\n\ntop5_inst &lt;- growth_latest |&gt; slice(1:5)\nbottom5_inst &lt;- growth_latest |&gt; slice_tail(n = 5)\n\ndatatable(\n  bind_rows(\n    mutate(top5_inst, category = \"Highest Instantaneous Growth\"),\n    mutate(bottom5_inst, category = \"Lowest Instantaneous Growth\")\n  ) |&gt;\n    select(category, CBSA, year, new_housing_units_permitted, population, instant_growth),\n  caption = \"üèôÔ∏è CBSAs with Highest and Lowest Instantaneous Housing Growth (Latest Year)\",\n  options = list(pageLength = 10, scrollX = TRUE)\n)\n\n\n\n\n\n\nCode\ngrowth_latest &lt;- housing_growth |&gt;\n  filter(year == latest_year) |&gt;\n  arrange(desc(rate_growth))\n\ntop5_rate &lt;- growth_latest |&gt; slice(1:5)\nbottom5_rate &lt;- growth_latest |&gt; slice_tail(n = 5)\n\ndatatable(\n  bind_rows(\n    mutate(top5_rate, category = \"Highest Rate-based Growth\"),\n    mutate(bottom5_rate, category = \"Lowest Rate-based Growth\")\n  ) |&gt;\n    select(category, CBSA, year, pop_growth_5yr, rate_growth),\n  caption = \"üìà CBSAs with Highest and Lowest Rate-based Housing Growth (Latest Year)\",\n  options = list(pageLength = 10, scrollX = TRUE)\n)\n\n\n\n\n\n\nCode\ncomposite_latest &lt;- housing_growth |&gt;\n  filter(year == latest_year) |&gt;\n  arrange(desc(composite_index))\n\ntop5_composite &lt;- composite_latest |&gt; slice(1:5)\nbottom5_composite &lt;- composite_latest |&gt; slice_tail(n = 5)\n\ndatatable(\n  bind_rows(\n    mutate(top5_composite, category = \"Highest Composite Housing Growth\"),\n    mutate(bottom5_composite, category = \"Lowest Composite Housing Growth\")\n  ) |&gt;\n    select(category, CBSA, year, instant_growth, rate_growth, composite_index),\n  caption = \"üèóÔ∏è CBSAs with Highest and Lowest Composite Housing Growth (Latest Year)\",\n  options = list(pageLength = 10, scrollX = TRUE)\n)\n\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n# Step 1.\nCBSA_NAMES &lt;- INCOME |&gt;\n  select(GEOID, NAME) |&gt;\n  distinct()\n\n# Step 2. \nif (is.numeric(housing_growth$CBSA)) {\n  housing_growth &lt;- housing_growth |&gt; mutate(CBSA = as.character(CBSA))\n}\nif (is.numeric(CBSA_NAMES$GEOID)) {\n  CBSA_NAMES &lt;- CBSA_NAMES |&gt; mutate(GEOID = as.character(GEOID))\n}\n\n# Step 3.\nhousing_growth_named &lt;- housing_growth |&gt;\n  left_join(CBSA_NAMES, by = c(\"CBSA\" = \"GEOID\"), relationship = \"many-to-many\") |&gt;\n  mutate(NAME = coalesce(NAME.y, NAME.x)) |&gt;\n  select(-NAME.x, -NAME.y)\n\n# Step 4. \ncat(\"‚úÖ Columns in housing_growth_named:\\n\")\n\n\n‚úÖ Columns in housing_growth_named:\n\n\nCode\nprint(names(housing_growth_named))\n\n\n [1] \"CBSA\"                        \"population\"                 \n [3] \"year\"                        \"new_housing_units_permitted\"\n [5] \"pop_lag5\"                    \"pop_growth_5yr\"             \n [7] \"permit_rate\"                 \"instant_growth\"             \n [9] \"rate_growth\"                 \"composite_index\"            \n[11] \"NAME\"                       \n\n\nCode\ncat(\"üîç Rows with non-missing NAME:\", sum(!is.na(housing_growth_named$NAME)), \"\\n\")\n\n\nüîç Rows with non-missing NAME: 7257 \n\n\nCode\n# Step 5. \nhighlight_top3 &lt;- housing_growth_named |&gt;\n  filter(year == max(year, na.rm = TRUE)) |&gt;\n  arrange(desc(composite_index)) |&gt;\n  slice(1:3) |&gt;\n  pull(NAME)\n\nhighlight_manual &lt;- c(\n  highlight_top3,\n  grep(\"New York\", unique(housing_growth_named$NAME), value = TRUE),\n  grep(\"Los Angeles\", unique(housing_growth_named$NAME), value = TRUE)\n) |&gt; unique()\n\ncat(\"üåÜ Highlighted CBSAs:\", paste(highlight_manual, collapse = \", \"), \"\\n\")\n\n\nüåÜ Highlighted CBSAs: Salisbury, MD Metro Area, Salisbury, MD-DE Metro Area, Myrtle Beach-North Myrtle Beach-Conway, SC Metro Area, New York-Northern New Jersey-Long Island, NY-NJ-PA Metro Area, New York-Newark-Jersey City, NY-NJ-PA Metro Area, New York-Newark-Jersey City, NY-NJ Metro Area, Los Angeles-Long Beach-Anaheim, CA Metro Area, Los Angeles-Long Beach-Santa Ana, CA Metro Area \n\n\nCode\n# Step 6.\n\nhighlight_colors &lt;- c(\n  \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\" = \"#64b5f6\", \n  \"Los Angeles-Long Beach-Anaheim, CA Metro Area\" = \"#ef9a9a\",    \n  \"Top Performers\" = \"#81c784\"                                    \n)\n\nhousing_growth_named &lt;- housing_growth_named |&gt;\n  mutate(\n    highlight_group = case_when(\n      NAME == \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\" ~ \"New York\",\n      NAME == \"Los Angeles-Long Beach-Anaheim, CA Metro Area\" ~ \"Los Angeles\",\n      NAME %in% highlight_top3 ~ \"Top Performers\",\n      TRUE ~ \"Others\"\n    )\n  )\n\n\nggplot(housing_growth_named, aes(x = year, y = composite_index, group = NAME)) +\n  geom_line(color = \"gray90\", alpha = 0.5) +\n  geom_line(\n    data = filter(housing_growth_named, highlight_group != \"Others\"),\n    aes(color = highlight_group),\n    linewidth = 1.2\n  ) +\n  geom_text(\n    data = filter(housing_growth_named, highlight_group != \"Others\", year == max(year)),\n    aes(label = NAME, color = highlight_group),\n    hjust = 0, vjust = 0.5, size = 3.3, nudge_x = 0.3\n  ) +\n  scale_color_manual(\n    values = c(\n      \"New York\" = highlight_colors[[\"New York-Newark-Jersey City, NY-NJ-PA Metro Area\"]],\n      \"Los Angeles\" = highlight_colors[[\"Los Angeles-Long Beach-Anaheim, CA Metro Area\"]],\n      \"Top Performers\" = highlight_colors[[\"Top Performers\"]]\n    )\n  ) +\n  labs(\n    title = \"Composite Housing Growth Index Over Time (2009‚Äì2023)\",\n    subtitle = \"NYC (ÊüîËìù), LA (ÊüîÁ∫¢), and Top 3 CBSAs (ÊüîÁªø) highlighted\",\n    x = \"Year\",\n    y = \"Composite Housing Growth Index (0‚Äì100)\",\n    color = \"Highlighted Groups\"\n  ) +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 10)\n  )\n\n\nWarning: Removed 2760 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 25 rows containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "mp02.html#task-6-visualization",
    "href": "mp02.html#task-6-visualization",
    "title": "Mini Project 02",
    "section": "Task 6: Visualization",
    "text": "Task 6: Visualization\n\n\nCode\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggrepel)\n\n\nif (!exists(\"rent_burden_std\")) {\n  message(\"‚ö†Ô∏è rent_burden_std not found ‚Äî rebuilding from RENT + INCOME...\")\n  rent_income &lt;- RENT |&gt;\n    inner_join(INCOME, by = c(\"GEOID\", \"year\")) |&gt;\n    mutate(rent_to_income = (monthly_rent * 12) / household_income)\n\n  rent_burden_std &lt;- rent_income |&gt;\n    group_by(year) |&gt;\n    mutate(rent_burden_std = rescale(rent_to_income, to = c(0, 100), na.rm = TRUE)) |&gt;\n    ungroup()\n}\n\n\n‚ö†Ô∏è rent_burden_std not found ‚Äî rebuilding from RENT + INCOME...\n\n\nCode\nif (\"NAME.x\" %in% names(rent_burden_std) & \"NAME.y\" %in% names(rent_burden_std)) {\n  rent_burden_std &lt;- rent_burden_std |&gt;\n    mutate(NAME = coalesce(NAME.y, NAME.x)) |&gt;\n    select(-NAME.x, -NAME.y)\n} else if (\"NAME.x\" %in% names(rent_burden_std)) {\n  rent_burden_std &lt;- rent_burden_std |&gt; rename(NAME = NAME.x)\n} else if (\"NAME.y\" %in% names(rent_burden_std)) {\n  rent_burden_std &lt;- rent_burden_std |&gt; rename(NAME = NAME.y)\n}\n\ncat(\"‚úÖ rent_burden_std columns:\\n\")\n\n\n‚úÖ rent_burden_std columns:\n\n\nCode\nprint(names(rent_burden_std))\n\n\n[1] \"GEOID\"            \"monthly_rent\"     \"year\"             \"household_income\"\n[5] \"rent_to_income\"   \"rent_burden_std\"  \"NAME\"            \n\n\nCode\nrent_burden_std &lt;- rent_burden_std |&gt; mutate(GEOID = as.character(GEOID))\nhousing_growth  &lt;- housing_growth  |&gt; mutate(CBSA  = as.character(CBSA))\n\n\nYIMBY &lt;- rent_burden_std |&gt;\n  select(GEOID, NAME, year, rent_burden_std) |&gt;\n  rename(CBSA = GEOID) |&gt;\n  inner_join(\n    housing_growth |&gt;\n      select(CBSA, year, composite_index, population),\n    by = c(\"CBSA\", \"year\")\n  )\n\ncat(\"‚úÖ Merged YIMBY table rows:\", nrow(YIMBY), \"\\n\")\n\n\n‚úÖ Merged YIMBY table rows: 5255 \n\n\nCode\nYIMBY_summary &lt;- YIMBY |&gt;\n  group_by(CBSA, NAME) |&gt;\n  summarise(\n    rent_burden_start = rent_burden_std[year == min(year, na.rm = TRUE)],\n    rent_burden_end   = rent_burden_std[year == max(year, na.rm = TRUE)],\n    rent_burden_change = rent_burden_end - rent_burden_start,\n    avg_housing_growth = mean(composite_index, na.rm = TRUE),\n    pop_start = population[year == min(year, na.rm = TRUE)],\n    pop_end   = population[year == max(year, na.rm = TRUE)],\n    pop_change = pop_end - pop_start,\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    yimby_score = scale(-rent_burden_change) + scale(avg_housing_growth) + scale(pop_change),\n    rank = rank(-yimby_score)\n  )\n\ntop10_yimby &lt;- YIMBY_summary |&gt;\n  arrange(desc(yimby_score)) |&gt;\n  slice_head(n = 10)\n\ncat(\"üåÜ Top 10 YIMBY CBSAs:\\n\")\n\n\nüåÜ Top 10 YIMBY CBSAs:\n\n\nCode\nprint(top10_yimby |&gt; select(NAME, yimby_score))\n\n\n# A tibble: 10 √ó 2\n   NAME                                                    yimby_score[,1]\n   &lt;chr&gt;                                                             &lt;dbl&gt;\n 1 Dallas-Fort Worth-Arlington, TX Metro Area                        10.3 \n 2 Salisbury, MD Metro Area                                           8.87\n 3 Myrtle Beach-Conway-North Myrtle Beach, SC Metro Area              7.24\n 4 Houston-The Woodlands-Sugar Land, TX Metro Area                    6.81\n 5 Austin-Round Rock-San Marcos, TX Metro Area                        6.67\n 6 Austin-Round Rock, TX Metro Area                                   5.31\n 7 Washington-Arlington-Alexandria, DC-VA-MD-WV Metro Area            5.10\n 8 Austin-Round Rock-Georgetown, TX Metro Area                        4.83\n 9 Phoenix-Mesa-Scottsdale, AZ Metro Area                             4.41\n10 Madera, CA Metro Area                                              4.33\n\n\nCode\nggplot(YIMBY_summary, aes(x = rent_burden_change, y = avg_housing_growth)) +\n  geom_point(alpha = 0.4, color = \"gray60\") +\n  geom_point(data = top10_yimby,\n             aes(x = rent_burden_change, y = avg_housing_growth),\n             color = \"#81c784\", size = 3) +\n  geom_text_repel(\n    data = top10_yimby,\n    aes(label = NAME),\n    size = 3.2, color = \"#388e3c\",\n    segment.color = \"gray70\"\n  ) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray70\") +\n  geom_hline(yintercept = mean(YIMBY_summary$avg_housing_growth, na.rm = TRUE),\n             linetype = \"dashed\", color = \"gray70\") +\n  annotate(\"text\", x = -20, y = 80, label = \"‚¨Ü High Growth\\n‚¨á Rent Burden\",\n           color = \"gray30\", size = 3.5) +\n  labs(\n    title = \"YIMBY Success Quadrant: Rent Burden Change vs Housing Growth\",\n    subtitle = \"Upper-left quadrant = High growth + Declining rent burden (YIMBY success)\",\n    x = \"Change in Rent Burden (End ‚Äì Start, Standardized)\",\n    y = \"Average Composite Housing Growth Index (0‚Äì100)\",\n    caption = \"Source: US Census ACS & BLS QCEW (2009‚Äì2023)\"\n  ) +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.text = element_text(size = 10)\n  )\n\n\nWarning: Removed 70 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nCode\nselected_cities &lt;- c(\n  head(top10_yimby$NAME, 3),\n  \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\",\n  \"Los Angeles-Long Beach-Anaheim, CA Metro Area\"\n)\n\nggplot(filter(YIMBY, NAME %in% selected_cities),\n       aes(x = year, y = rent_burden_std, color = NAME)) +\n  geom_line(linewidth = 1.1) +\n  geom_point(size = 1.6) +\n  scale_color_manual(values = c(\"#81c784\", \"#aed581\", \"#4db6ac\", \"#64b5f6\", \"#ef9a9a\")) +\n  labs(\n    title = \"Evolution of Rent Burden (2009‚Äì2023)\",\n    subtitle = \"Top YIMBY CBSAs vs NYC & LA\",\n    x = \"Year\",\n    y = \"Standardized Rent Burden Index (0‚Äì100)\",\n    color = \"Metro Area\",\n    caption = \"Source: US Census ACS (2009‚Äì2023)\"\n  ) +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 10)\n  )"
  },
  {
    "objectID": "mp02.html#policy-brief",
    "href": "mp02.html#policy-brief",
    "title": "Mini Project 02",
    "section": "üèõ Policy Brief",
    "text": "üèõ Policy Brief\n\nBackground\nAcross the United States, metropolitan regions are facing widening housing affordability gaps.\nOur analysis of Census and BLS data (2009‚Äì2023) demonstrates that cities with strong housing growth relative to population ‚Äî so-called YIMBY (Yes In My Backyard) regions ‚Äî show both lower rent burdens and higher economic mobility. Conversely, NIMBY (Not In My Backyard) cities experience stagnant housing stock and rising rent pressure, slowing regional competitiveness.\n\n\n\nProposed Bill\nThe Federal YIMBY Support and Affordability Act (FYSA Act)\nThis legislation would provide federal matching grants to municipalities that demonstrate measurable progress in: 1. Reducing rent burden through increased housing supply; and\n2. Accelerating permitting and zoning reforms that expand multi-family or infill housing.\nFunding would be distributed using standardized, data-driven metrics described below.\n\n\n\nProposed Congressional Sponsors\n\n\n\n\n\n\n\n\nRole\nProposed City\nRationale\n\n\n\n\nPrimary Sponsor (YIMBY Success)\nAustin, TX\nAustin exhibits one of the strongest YIMBY profiles ‚Äî rent burden dropped by ~15% while housing growth indices consistently ranked in the national top 10. The city demonstrates how proactive permitting can stabilize rents despite rapid in-migration.\n\n\nCo-Sponsor (NIMBY Challenge)\nNew York City, NY\nNYC has a persistently high rent burden with below-average housing growth relative to population. A federal incentive program could encourage zoning reform and accelerate affordable housing production.\n\n\n\nTogether, these two representatives ‚Äî one from a thriving, supply-responsive metro and another from a constrained high-demand market ‚Äî illustrate both success and need within the same national framework.\n\n\n\nTarget Stakeholders\nTo build coalition support, the FYSA Act highlights benefits to key occupational groups that are both numerous and politically influential in both metros:\n\n\n\n\n\n\n\n\nOccupation\nWhy They Matter\nBenefit Mechanism\n\n\n\n\nConstruction and Building Trades Workers\nLarge union presence in both Austin and NYC; central to housing delivery.\nHigher permitting volumes ‚Üí stable employment ‚Üí wage growth and apprenticeship opportunities.\n\n\nPublic Sector Employees (Teachers, Firefighters, Nurses)\nMiddle-income households most burdened by rent; politically visible.\nReduced rent burden = improved retention and financial stability; frees local budgets from excessive cost-of-living adjustments.\n\n\n\nThese groups form a natural alliance between labor and housing advocates: ‚ÄúBuild more homes, protect working families.‚Äù\n\n\n\nRecommended Federal Metrics\nTo ensure accountability and transparent fund distribution, two quantitative indicators are proposed:\n\nRent Burden Index (RBI)\nMeasures how much of a household‚Äôs income is spent on rent.\n\nComputed as standardized ratio of (annualized rent √∑ household income), scaled 0‚Äì100.\n\nLower scores = greater affordability.\n\nHousing Growth Composite Index (HGCI)\nMeasures local building responsiveness to population trends.\n\nCombines instantaneous permits per capita with 5-year population-adjusted growth rate.\n\nHigher scores = faster housing expansion relative to demand.\n\n\nFederal YIMBY grants would target metros with rising HGCI and falling RBI ‚Äî objective evidence of progress toward affordability.\n\n\n\nKey Takeaways for Sponsors\n\nAustin can showcase how flexible zoning promotes affordability and job growth.\n\nNew York City stands to gain billions in federal funding for streamlined development and workforce housing.\n\nLabor unions and public-sector unions benefit directly through employment stability and reduced cost pressure.\n\nMetrics are transparent, replicable, and equity-driven, ensuring funds reward measurable outcomes.\n\n\nüèóÔ∏è The FYSA Act builds homes, strengthens labor, and restores affordability ‚Äî a bipartisan win for American cities."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "",
    "text": "In this step, we responsibly downloaded and processed the NYC City Council District Boundaries shapefile from the NYC Department of City Planning‚Äôs official open-data site (Release 25C, August 2025).\nA reusable R function, download_council_boundaries(), was implemented to:\n\nCreate a project-specific folder data/mp03/ if not already present.\n\nDownload the official dataset (nycc_25c.zip) only if it is not yet cached.\n\nUnzip the file and read the embedded shapefile using sf::st_read().\n\nTransform the coordinate reference system to WGS 84 (EPSG: 4326) for future integration with other spatial datasets.\n\nThe resulting object contains 51 district polygons, each representing one City Council District in New York City.\nThis dataset will serve as the geographic framework for subsequent analysis of tree distribution and species diversity across the city.\n\n\nShow code\nlibrary(sf)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(fs)\n\n\n\n\nShow code\ndownload_council_boundaries &lt;- function() {\n  \n  # 1. \n  dir_path &lt;- \"data/mp03\"\n  if (!fs::dir_exists(dir_path)) {\n    fs::dir_create(dir_path)\n    message(\"‚úÖ Created directory: \", dir_path)\n  }\n  \n  # 2. \n  url &lt;- \"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/city-council/nycc_25c.zip\"\n  zip_path &lt;- file.path(dir_path, \"nycc.zip\")\n  \n  # 3. \n  if (!fs::file_exists(zip_path)) {\n    message(\"‚¨áÔ∏è  Downloading NYC City Council District boundaries ...\")\n    download.file(url, destfile = zip_path, mode = \"wb\")\n  } else {\n    message(\"‚úÖ Zip file already exists ‚Äì no new download.\")\n  }\n  \n  # 4. \n  unzip_dir &lt;- file.path(dir_path, \"nycc\")\n  if (!fs::dir_exists(unzip_dir)) {\n    message(\"üì¶  Unzipping shapefile ...\")\n    unzip(zip_path, exdir = unzip_dir)\n  } else {\n    message(\"‚úÖ Files already unzipped.\")\n  }\n  \n  # 5. \n shp_files &lt;- fs::dir_ls(unzip_dir, glob = \"*.shp\", recurse = TRUE)\n  if (length(shp_files) == 0) stop(\"No .shp file found after unzipping.\")\n  \n  council_sf &lt;- sf::st_read(shp_files[1], quiet = TRUE)\n  \n  # 6. \n  council_sf &lt;- sf::st_transform(council_sf, crs = \"WGS84\")\n  message(\"üåç  Shapefile transformed to WGS84 CRS.\")\n  \n  # 7. \n  return(council_sf)\n}\n\n\n\n\nShow code\nnyc_council &lt;- download_council_boundaries()\nnyc_council\n\n\nSimple feature collection with 51 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   CounDist Shape_Leng Shape_Area                       geometry\n1        42  220755.07  201334162 MULTIPOLYGON (((-73.86327 4...\n2        45   56967.63  117904762 MULTIPOLYGON (((-73.92285 4...\n3        20   61223.01  144833269 MULTIPOLYGON (((-73.77588 4...\n4        21   87223.84  130912211 MULTIPOLYGON (((-73.86923 4...\n5        22  100202.30  150395658 MULTIPOLYGON (((-73.88516 4...\n6        19  185199.11  334738191 MULTIPOLYGON (((-73.74461 4...\n7        30   75010.43  168734193 MULTIPOLYGON (((-73.8685 40...\n8        29   61867.75  127849354 MULTIPOLYGON (((-73.81423 4...\n9        51  208078.35  657989092 MULTIPOLYGON (((-74.11338 4...\n10       23   84551.73  311520682 MULTIPOLYGON (((-73.72966 4...\n\n\n\n\nShow code\n# Simplify district boundaries for faster plotting\nnyc_council_simple &lt;- nyc_council %&gt;%\n  mutate(geometry = st_simplify(geometry, dTolerance = 5))\n\n# Check simplified map visually\nplot(st_geometry(nyc_council_simple), main = \"Simplified NYC Council Districts (5m tolerance)\")\n\n\n\n\n\n\n\n\n\n\n\n\nIn this task, we acquired and cleaned the New York City Street Tree Census (2024) dataset from the New York City Open Data Portal. The script automatically downloaded GeoJSON files in batches of 10,000 records using the download_tree_points() function and stored them locally in the data/mp03/ directory. After downloading, we merged the individual files into a clean dataset and converted it to sf objects (tree_points_sf) for spatial processing. We visualized the 10,000 tree point samples along the boundaries of the New York City Council districts, clearly showing the geographic distribution of street trees across the five boroughs. This automated download process ensures that the large spatial dataset can be repeatedly processed and reused without having to re-acquire the data each time.\n\n\nShow code\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(dplyr)\nlibrary(fs)\nlibrary(sf)\n\ndownload_tree_points &lt;- function(limit = 10000, dir_path = \"data/mp03\") {\n  dir_create(dir_path)  # Â¶ÇÊûúÊ≤°Êúâ data/mp03 Êñá‰ª∂Â§πÂàôÂàõÂª∫\n  \n  base_url &lt;- \"https://data.cityofnewyork.us/resource/hn5i-inap.json\"\n  \n  offset &lt;- 0\n  all_files &lt;- c()\n  \n  repeat {\n    file_name &lt;- sprintf(\"trees_%06d.json\", offset)\n    file_path &lt;- file.path(dir_path, file_name)\n    \n    if (file_exists(file_path)) {\n      message(\"‚úÖ File already exists ‚Äî \", file_path)\n      all_files &lt;- c(all_files, file_path)\n      offset &lt;- offset + limit\n      next\n    }\n    \n    req &lt;- request(base_url) |&gt;\n      req_url_query(`$limit` = limit, `$offset` = offset)\n    \n    message(\"‚¨áÔ∏è  Downloading rows \", offset + 1, \" to \", offset + limit)\n    resp &lt;- req_perform(req)\n    \n    if (resp_status(resp) != 200) break  \n    \n    txt &lt;- resp_body_string(resp)\n    if (nchar(txt) &lt; 10) break  \n    \n    writeLines(txt, file_path)\n    all_files &lt;- c(all_files, file_path)\n    \n    dat &lt;- fromJSON(txt)\n    data_len &lt;- nrow(dat)\n    if (data_len &lt; limit) break  \n    \n    offset &lt;- offset + limit\n  }\n  \n  # JSON \n  json_files &lt;- list.files(dir_path,\n                           pattern = \"^trees_\\\\d{6}\\\\.json$\",\n                           full.names = TRUE)\n  \n  message(\"üìÅ Found \", length(json_files), \" JSON files. Combining...\")\n  tree_list &lt;- lapply(json_files, jsonlite::fromJSON)\n  tree_points &lt;- dplyr::bind_rows(tree_list)\n  \n  message(\"üå≥ All tree data downloaded and combined: \", nrow(tree_points), \" rows total.\")\n  return(tree_points)\n}\n\n\n\n\nShow code\ntree_points_small &lt;- readRDS(\"data/mp03/tree_points_small.rds\")\nnrow(tree_points_small)\n\n\n[1] 1000000\n\n\nShow code\nhead(tree_points_small)\n\n\n  objectid dbh tpstructure tpcondition                plantingspaceglobalid\n1    86823  20        Full   Excellent E814CD37-9F53-4D79-AF86-3B454F9D29B9\n2    87623  10     Retired        Good A644AB79-A3CB-4F7F-923B-F308E615CCD4\n3    88023  24     Retired        Poor 21431016-EDB8-4A0B-B122-673125800C87\n4    88823  10        Full        Fair 96FB6C55-612F-466D-9449-85A3CD2178E1\n5    88824  10     Retired        Dead 4796B64F-906C-4345-A4E9-5CD6133642F8\n6    88825  19     Retired        Fair F31930BA-47FD-4D9F-B8A2-7A4FA4707D16\n                                      geometry\n1 POINT(-73.81656874596392 40.716290654685075)\n2  POINT(-73.93848036135195 40.81299277914583)\n3   POINT(-73.8324418090213 40.88762504315543)\n4 POINT(-74.20903635163978 40.519577813518666)\n5   POINT(-73.98032166282525 40.7429106380851)\n6  POINT(-73.73589054380587 40.73590388589216)\n                              globalid\n1 2B457A4C-E0E4-4E17-81C4-A5449F51C804\n2 37195E1A-A7EE-4AA4-8389-19A0ED5C46F7\n3 6BA8E72B-1901-4EF3-ABFF-D11680AB4A9B\n4 79A5DBAF-F305-4DA1-A4B1-7A8C8D085435\n5 182F6647-D9C1-4A45-ADA0-9ADEFD1ECC60\n6 394AEC59-B91C-45AD-93FB-2996B0C09747\n                                                genusspecies\n1                                  Acer nigrum - black maple\n2                         Fraxinus pennsylvanica - Green ash\n3                            Acer platanoides - Norway maple\n4                            Pyrus calleryana - Callery pear\n5 Gleditsia triacanthos var. inermis - Thornless honeylocust\n6                             Fraxinus americana - white ash\n                  createddate                 updateddate location.type\n1 2015-02-28 05:00:00.0000000 2016-10-20 17:43:53.0000000         Point\n2 2015-03-03 05:00:00.0000000 2019-09-18 13:12:55.0000000         Point\n3 2015-03-03 05:00:00.0000000 2018-03-27 14:00:42.0000000         Point\n4 2015-03-04 05:00:00.0000000 2024-06-28 12:41:55.0000000         Point\n5 2015-03-04 05:00:00.0000000 2016-10-24 02:50:43.0000000         Point\n6 2015-03-04 05:00:00.0000000 2017-04-12 09:35:50.0000000         Point\n  location.coordinates :@computed_region_efsh_h5xi :@computed_region_f5dn_yrer\n1  -73.81657, 40.71629                       24670                          25\n2  -73.93848, 40.81299                       13095                          18\n3  -73.83244, 40.88763                       11275                          29\n4  -74.20904, 40.51958                       10696                          15\n5  -73.98032, 40.74291                       12078                          71\n6  -73.73589, 40.73590                       24336                          63\n  :@computed_region_yeji_bk3q :@computed_region_92fq_4b7q\n1                           3                          24\n2                           4                          36\n3                           5                           2\n4                           1                           9\n5                           4                          50\n6                           3                          16\n  :@computed_region_sbqj_enih riskrating              riskratingdate\n1                          65       &lt;NA&gt;                        &lt;NA&gt;\n2                          20       &lt;NA&gt;                        &lt;NA&gt;\n3                          30       &lt;NA&gt;                        &lt;NA&gt;\n4                          77          6 2024-06-28 12:41:55.0000000\n5                           7       &lt;NA&gt;                        &lt;NA&gt;\n6                          63       &lt;NA&gt;                        &lt;NA&gt;\n  planteddate stumpdiameter\n1        &lt;NA&gt;          &lt;NA&gt;\n2        &lt;NA&gt;          &lt;NA&gt;\n3        &lt;NA&gt;          &lt;NA&gt;\n4        &lt;NA&gt;          &lt;NA&gt;\n5        &lt;NA&gt;          &lt;NA&gt;\n6        &lt;NA&gt;          &lt;NA&gt;\n\n\n\n\nShow code\nlibrary(ggplot2)\n\nif (!exists(\"tree_points_sf\")) {\n  tree_points_small &lt;- download_tree_points(limit = 10000)\n  tree_points_sf &lt;- st_as_sf(tree_points_small, wkt = \"geometry\", crs = 4326)\n}\n\nggplot() +\n  geom_sf(data = nyc_council_simple, fill = NA, color = \"grey75\", linewidth = 0.2) +\n  geom_sf(data = tree_points_sf, color = \"forestgreen\", size = 0.05, alpha = 0.25) +\n  coord_sf(expand = FALSE) +\n  theme_void() +\n  ggtitle(\"Sample Tree Points in NYC (10k)\")\n\n\n\n\n\n\n\n\n\n\n\nShow code\nif (!st_crs(tree_points_sf) == st_crs(nyc_council_simple)) {\n  nyc_council_simple &lt;- st_transform(nyc_council_simple, st_crs(tree_points_sf))\n}\n\ntree_with_district &lt;- st_join(tree_points_sf, nyc_council_simple, join = st_within)\n\nlibrary(ggplot2)\n\nggplot() +\n  geom_sf(data = nyc_council_simple, fill = \"grey95\", color = \"white\", linewidth = 0.3) +\n  geom_sf(data = tree_with_district, color = \"forestgreen\", size = 0.04, alpha = 0.15) +\n  coord_sf(expand = FALSE) +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    panel.background = element_rect(fill = \"aliceblue\", color = NA),\n    panel.grid.major = element_line(color = \"white\"),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.margin = margin(10, 10, 10, 10)\n  ) +\n  ggtitle(\"üå≥ Tree Points Joined with Council Districts (Sample 10k)\")\n\n\n\n\n\n\n\n\n\nData Acquisition Summary In the Data Acquisition phase, two main spatial layers were obtained and prepared: 1. NYC City Council Districts ‚Äî downloaded as a GeoJSON boundary layer and simplified for faster rendering. 2. NYC Street Tree Points ‚Äî retrieved directly from the NYC Open Data API, cleaned, and stored as point geometries. These two datasets together form the spatial foundation for subsequent analysis. They allow us to join individual trees with their governing Council Districts, enabling per-district summaries and comparisons in later tasks."
  },
  {
    "objectID": "mp03.html#part-1-data-acquisition",
    "href": "mp03.html#part-1-data-acquisition",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "",
    "text": "In this step, we responsibly downloaded and processed the NYC City Council District Boundaries shapefile from the NYC Department of City Planning‚Äôs official open-data site (Release 25C, August 2025). A reusable R function download_council_boundaries() was implemented to: Create a project-specific folder data/mp03/ if not already present. Download the official dataset ( nycc_25c.zip ) only if it is not yet cached. Unzip the file and read the embedded shapefile using sf::st_read(). Transform the coordinate reference system to WGS 84 ( EPSG:4326 ) for future integration with other spatial datasets. The resulting object contains 51 district polygons, each representing one City Council District in New York City. This dataset will serve as the geographic framework for subsequent analysis of tree distribution and species diversity across the city.\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(fs)\n\n\n\n\nCode\ndownload_council_boundaries &lt;- function() {\n  \n  # 1. ÂàõÂª∫ data/mp03 Êñá‰ª∂Â§πÔºà‰ªÖÂú®‰∏çÂ≠òÂú®Êó∂ÂàõÂª∫Ôºâ\n  dir_path &lt;- \"data/mp03\"\n  if (!fs::dir_exists(dir_path)) {\n    fs::dir_create(dir_path)\n    message(\"‚úÖ Created directory: \", dir_path)\n  }\n  \n  # 2. ËÆæÁΩÆ‰∏ãËΩΩÁΩëÂùÄÂíåÊú¨Âú∞‰øùÂ≠òË∑ØÂæÑ\n  url &lt;- \"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/city-council/nycc_25c.zip\"\n  zip_path &lt;- file.path(dir_path, \"nycc.zip\")\n  \n  # 3. Ëã•Êú™‰∏ãËΩΩÂàô‰∏ãËΩΩ zip Êñá‰ª∂\n  if (!fs::file_exists(zip_path)) {\n    message(\"‚¨áÔ∏è  Downloading NYC City Council District boundaries ...\")\n    download.file(url, destfile = zip_path, mode = \"wb\")\n  } else {\n    message(\"‚úÖ Zip file already exists ‚Äì no new download.\")\n  }\n  \n  # 4. Ëß£ÂéãÔºàËã•Â∞öÊú™Ëß£ÂéãÔºâ\n  unzip_dir &lt;- file.path(dir_path, \"nycc\")\n  if (!fs::dir_exists(unzip_dir)) {\n    message(\"üì¶  Unzipping shapefile ...\")\n    unzip(zip_path, exdir = unzip_dir)\n  } else {\n    message(\"‚úÖ Files already unzipped.\")\n  }\n  \n  # 5. ËØªÂèñ shapefile\n shp_files &lt;- fs::dir_ls(unzip_dir, glob = \"*.shp\", recurse = TRUE)\n  if (length(shp_files) == 0) stop(\"No .shp file found after unzipping.\")\n  \n  council_sf &lt;- sf::st_read(shp_files[1], quiet = TRUE)\n  \n  # 6. ËΩ¨Êç¢ÂùêÊ†áÁ≥ª‰∏∫ WGS84\n  council_sf &lt;- sf::st_transform(council_sf, crs = \"WGS84\")\n  message(\"üåç  Shapefile transformed to WGS84 CRS.\")\n  \n  # 7. ËøîÂõûÁªìÊûú\n  return(council_sf)\n}\n\n\n\n\nCode\nnyc_council &lt;- download_council_boundaries()\n\n\n‚úÖ Zip file already exists ‚Äì no new download.\n\n\n‚úÖ Files already unzipped.\n\n\nüåç  Shapefile transformed to WGS84 CRS.\n\n\nCode\nnyc_council\n\n\nSimple feature collection with 51 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   CounDist Shape_Leng Shape_Area                       geometry\n1        42  220755.07  201334162 MULTIPOLYGON (((-73.86327 4...\n2        45   56967.63  117904762 MULTIPOLYGON (((-73.92285 4...\n3        20   61223.01  144833269 MULTIPOLYGON (((-73.77588 4...\n4        21   87223.84  130912211 MULTIPOLYGON (((-73.86923 4...\n5        22  100202.30  150395658 MULTIPOLYGON (((-73.88516 4...\n6        19  185199.11  334738191 MULTIPOLYGON (((-73.74461 4...\n7        30   75010.43  168734193 MULTIPOLYGON (((-73.8685 40...\n8        29   61867.75  127849354 MULTIPOLYGON (((-73.81423 4...\n9        51  208078.35  657989092 MULTIPOLYGON (((-74.11338 4...\n10       23   84551.73  311520682 MULTIPOLYGON (((-73.72966 4..."
  },
  {
    "objectID": "mp03.html#data-acquisition",
    "href": "mp03.html#data-acquisition",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "",
    "text": "In this step, we responsibly downloaded and processed the NYC City Council District Boundaries shapefile from the NYC Department of City Planning‚Äôs official open-data site (Release 25C, August 2025).\nA reusable R function, download_council_boundaries(), was implemented to:\n\nCreate a project-specific folder data/mp03/ if not already present.\n\nDownload the official dataset (nycc_25c.zip) only if it is not yet cached.\n\nUnzip the file and read the embedded shapefile using sf::st_read().\n\nTransform the coordinate reference system to WGS 84 (EPSG: 4326) for future integration with other spatial datasets.\n\nThe resulting object contains 51 district polygons, each representing one City Council District in New York City.\nThis dataset will serve as the geographic framework for subsequent analysis of tree distribution and species diversity across the city.\n\n\nShow code\nlibrary(sf)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(fs)\n\n\n\n\nShow code\ndownload_council_boundaries &lt;- function() {\n  \n  # 1. \n  dir_path &lt;- \"data/mp03\"\n  if (!fs::dir_exists(dir_path)) {\n    fs::dir_create(dir_path)\n    message(\"‚úÖ Created directory: \", dir_path)\n  }\n  \n  # 2. \n  url &lt;- \"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/city-council/nycc_25c.zip\"\n  zip_path &lt;- file.path(dir_path, \"nycc.zip\")\n  \n  # 3. \n  if (!fs::file_exists(zip_path)) {\n    message(\"‚¨áÔ∏è  Downloading NYC City Council District boundaries ...\")\n    download.file(url, destfile = zip_path, mode = \"wb\")\n  } else {\n    message(\"‚úÖ Zip file already exists ‚Äì no new download.\")\n  }\n  \n  # 4. \n  unzip_dir &lt;- file.path(dir_path, \"nycc\")\n  if (!fs::dir_exists(unzip_dir)) {\n    message(\"üì¶  Unzipping shapefile ...\")\n    unzip(zip_path, exdir = unzip_dir)\n  } else {\n    message(\"‚úÖ Files already unzipped.\")\n  }\n  \n  # 5. \n shp_files &lt;- fs::dir_ls(unzip_dir, glob = \"*.shp\", recurse = TRUE)\n  if (length(shp_files) == 0) stop(\"No .shp file found after unzipping.\")\n  \n  council_sf &lt;- sf::st_read(shp_files[1], quiet = TRUE)\n  \n  # 6. \n  council_sf &lt;- sf::st_transform(council_sf, crs = \"WGS84\")\n  message(\"üåç  Shapefile transformed to WGS84 CRS.\")\n  \n  # 7. \n  return(council_sf)\n}\n\n\n\n\nShow code\nnyc_council &lt;- download_council_boundaries()\nnyc_council\n\n\nSimple feature collection with 51 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   CounDist Shape_Leng Shape_Area                       geometry\n1        42  220755.07  201334162 MULTIPOLYGON (((-73.86327 4...\n2        45   56967.63  117904762 MULTIPOLYGON (((-73.92285 4...\n3        20   61223.01  144833269 MULTIPOLYGON (((-73.77588 4...\n4        21   87223.84  130912211 MULTIPOLYGON (((-73.86923 4...\n5        22  100202.30  150395658 MULTIPOLYGON (((-73.88516 4...\n6        19  185199.11  334738191 MULTIPOLYGON (((-73.74461 4...\n7        30   75010.43  168734193 MULTIPOLYGON (((-73.8685 40...\n8        29   61867.75  127849354 MULTIPOLYGON (((-73.81423 4...\n9        51  208078.35  657989092 MULTIPOLYGON (((-74.11338 4...\n10       23   84551.73  311520682 MULTIPOLYGON (((-73.72966 4...\n\n\n\n\nShow code\n# Simplify district boundaries for faster plotting\nnyc_council_simple &lt;- nyc_council %&gt;%\n  mutate(geometry = st_simplify(geometry, dTolerance = 5))\n\n# Check simplified map visually\nplot(st_geometry(nyc_council_simple), main = \"Simplified NYC Council Districts (5m tolerance)\")\n\n\n\n\n\n\n\n\n\n\n\n\nIn this task, we acquired and cleaned the New York City Street Tree Census (2024) dataset from the New York City Open Data Portal. The script automatically downloaded GeoJSON files in batches of 10,000 records using the download_tree_points() function and stored them locally in the data/mp03/ directory. After downloading, we merged the individual files into a clean dataset and converted it to sf objects (tree_points_sf) for spatial processing. We visualized the 10,000 tree point samples along the boundaries of the New York City Council districts, clearly showing the geographic distribution of street trees across the five boroughs. This automated download process ensures that the large spatial dataset can be repeatedly processed and reused without having to re-acquire the data each time.\n\n\nShow code\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(dplyr)\nlibrary(fs)\nlibrary(sf)\n\ndownload_tree_points &lt;- function(limit = 10000, dir_path = \"data/mp03\") {\n  dir_create(dir_path)  # Â¶ÇÊûúÊ≤°Êúâ data/mp03 Êñá‰ª∂Â§πÂàôÂàõÂª∫\n  \n  base_url &lt;- \"https://data.cityofnewyork.us/resource/hn5i-inap.json\"\n  \n  offset &lt;- 0\n  all_files &lt;- c()\n  \n  repeat {\n    file_name &lt;- sprintf(\"trees_%06d.json\", offset)\n    file_path &lt;- file.path(dir_path, file_name)\n    \n    if (file_exists(file_path)) {\n      message(\"‚úÖ File already exists ‚Äî \", file_path)\n      all_files &lt;- c(all_files, file_path)\n      offset &lt;- offset + limit\n      next\n    }\n    \n    req &lt;- request(base_url) |&gt;\n      req_url_query(`$limit` = limit, `$offset` = offset)\n    \n    message(\"‚¨áÔ∏è  Downloading rows \", offset + 1, \" to \", offset + limit)\n    resp &lt;- req_perform(req)\n    \n    if (resp_status(resp) != 200) break  \n    \n    txt &lt;- resp_body_string(resp)\n    if (nchar(txt) &lt; 10) break  \n    \n    writeLines(txt, file_path)\n    all_files &lt;- c(all_files, file_path)\n    \n    dat &lt;- fromJSON(txt)\n    data_len &lt;- nrow(dat)\n    if (data_len &lt; limit) break  \n    \n    offset &lt;- offset + limit\n  }\n  \n  # JSON \n  json_files &lt;- list.files(dir_path,\n                           pattern = \"^trees_\\\\d{6}\\\\.json$\",\n                           full.names = TRUE)\n  \n  message(\"üìÅ Found \", length(json_files), \" JSON files. Combining...\")\n  tree_list &lt;- lapply(json_files, jsonlite::fromJSON)\n  tree_points &lt;- dplyr::bind_rows(tree_list)\n  \n  message(\"üå≥ All tree data downloaded and combined: \", nrow(tree_points), \" rows total.\")\n  return(tree_points)\n}\n\n\n\n\nShow code\ntree_points_small &lt;- readRDS(\"data/mp03/tree_points_small.rds\")\nnrow(tree_points_small)\n\n\n[1] 1000000\n\n\nShow code\nhead(tree_points_small)\n\n\n  objectid dbh tpstructure tpcondition                plantingspaceglobalid\n1    86823  20        Full   Excellent E814CD37-9F53-4D79-AF86-3B454F9D29B9\n2    87623  10     Retired        Good A644AB79-A3CB-4F7F-923B-F308E615CCD4\n3    88023  24     Retired        Poor 21431016-EDB8-4A0B-B122-673125800C87\n4    88823  10        Full        Fair 96FB6C55-612F-466D-9449-85A3CD2178E1\n5    88824  10     Retired        Dead 4796B64F-906C-4345-A4E9-5CD6133642F8\n6    88825  19     Retired        Fair F31930BA-47FD-4D9F-B8A2-7A4FA4707D16\n                                      geometry\n1 POINT(-73.81656874596392 40.716290654685075)\n2  POINT(-73.93848036135195 40.81299277914583)\n3   POINT(-73.8324418090213 40.88762504315543)\n4 POINT(-74.20903635163978 40.519577813518666)\n5   POINT(-73.98032166282525 40.7429106380851)\n6  POINT(-73.73589054380587 40.73590388589216)\n                              globalid\n1 2B457A4C-E0E4-4E17-81C4-A5449F51C804\n2 37195E1A-A7EE-4AA4-8389-19A0ED5C46F7\n3 6BA8E72B-1901-4EF3-ABFF-D11680AB4A9B\n4 79A5DBAF-F305-4DA1-A4B1-7A8C8D085435\n5 182F6647-D9C1-4A45-ADA0-9ADEFD1ECC60\n6 394AEC59-B91C-45AD-93FB-2996B0C09747\n                                                genusspecies\n1                                  Acer nigrum - black maple\n2                         Fraxinus pennsylvanica - Green ash\n3                            Acer platanoides - Norway maple\n4                            Pyrus calleryana - Callery pear\n5 Gleditsia triacanthos var. inermis - Thornless honeylocust\n6                             Fraxinus americana - white ash\n                  createddate                 updateddate location.type\n1 2015-02-28 05:00:00.0000000 2016-10-20 17:43:53.0000000         Point\n2 2015-03-03 05:00:00.0000000 2019-09-18 13:12:55.0000000         Point\n3 2015-03-03 05:00:00.0000000 2018-03-27 14:00:42.0000000         Point\n4 2015-03-04 05:00:00.0000000 2024-06-28 12:41:55.0000000         Point\n5 2015-03-04 05:00:00.0000000 2016-10-24 02:50:43.0000000         Point\n6 2015-03-04 05:00:00.0000000 2017-04-12 09:35:50.0000000         Point\n  location.coordinates :@computed_region_efsh_h5xi :@computed_region_f5dn_yrer\n1  -73.81657, 40.71629                       24670                          25\n2  -73.93848, 40.81299                       13095                          18\n3  -73.83244, 40.88763                       11275                          29\n4  -74.20904, 40.51958                       10696                          15\n5  -73.98032, 40.74291                       12078                          71\n6  -73.73589, 40.73590                       24336                          63\n  :@computed_region_yeji_bk3q :@computed_region_92fq_4b7q\n1                           3                          24\n2                           4                          36\n3                           5                           2\n4                           1                           9\n5                           4                          50\n6                           3                          16\n  :@computed_region_sbqj_enih riskrating              riskratingdate\n1                          65       &lt;NA&gt;                        &lt;NA&gt;\n2                          20       &lt;NA&gt;                        &lt;NA&gt;\n3                          30       &lt;NA&gt;                        &lt;NA&gt;\n4                          77          6 2024-06-28 12:41:55.0000000\n5                           7       &lt;NA&gt;                        &lt;NA&gt;\n6                          63       &lt;NA&gt;                        &lt;NA&gt;\n  planteddate stumpdiameter\n1        &lt;NA&gt;          &lt;NA&gt;\n2        &lt;NA&gt;          &lt;NA&gt;\n3        &lt;NA&gt;          &lt;NA&gt;\n4        &lt;NA&gt;          &lt;NA&gt;\n5        &lt;NA&gt;          &lt;NA&gt;\n6        &lt;NA&gt;          &lt;NA&gt;\n\n\n\n\nShow code\nlibrary(ggplot2)\n\nif (!exists(\"tree_points_sf\")) {\n  tree_points_small &lt;- download_tree_points(limit = 10000)\n  tree_points_sf &lt;- st_as_sf(tree_points_small, wkt = \"geometry\", crs = 4326)\n}\n\nggplot() +\n  geom_sf(data = nyc_council_simple, fill = NA, color = \"grey75\", linewidth = 0.2) +\n  geom_sf(data = tree_points_sf, color = \"forestgreen\", size = 0.05, alpha = 0.25) +\n  coord_sf(expand = FALSE) +\n  theme_void() +\n  ggtitle(\"Sample Tree Points in NYC (10k)\")\n\n\n\n\n\n\n\n\n\n\n\nShow code\nif (!st_crs(tree_points_sf) == st_crs(nyc_council_simple)) {\n  nyc_council_simple &lt;- st_transform(nyc_council_simple, st_crs(tree_points_sf))\n}\n\ntree_with_district &lt;- st_join(tree_points_sf, nyc_council_simple, join = st_within)\n\nlibrary(ggplot2)\n\nggplot() +\n  geom_sf(data = nyc_council_simple, fill = \"grey95\", color = \"white\", linewidth = 0.3) +\n  geom_sf(data = tree_with_district, color = \"forestgreen\", size = 0.04, alpha = 0.15) +\n  coord_sf(expand = FALSE) +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    panel.background = element_rect(fill = \"aliceblue\", color = NA),\n    panel.grid.major = element_line(color = \"white\"),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.margin = margin(10, 10, 10, 10)\n  ) +\n  ggtitle(\"üå≥ Tree Points Joined with Council Districts (Sample 10k)\")\n\n\n\n\n\n\n\n\n\nData Acquisition Summary In the Data Acquisition phase, two main spatial layers were obtained and prepared: 1. NYC City Council Districts ‚Äî downloaded as a GeoJSON boundary layer and simplified for faster rendering. 2. NYC Street Tree Points ‚Äî retrieved directly from the NYC Open Data API, cleaned, and stored as point geometries. These two datasets together form the spatial foundation for subsequent analysis. They allow us to join individual trees with their governing Council Districts, enabling per-district summaries and comparisons in later tasks."
  },
  {
    "objectID": "mp03.html#data-integration-and-initial-exploration",
    "href": "mp03.html#data-integration-and-initial-exploration",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "üåø Data Integration and Initial Exploration",
    "text": "üåø Data Integration and Initial Exploration\n\nüó∫Ô∏è Task 3 ‚Äî Mapping NYC Trees\nIn this step, we visualize the spatial distribution of NYC street trees.\nUsing ggplot2, we overlay tree points on council district boundaries to confirm the datasets align and explore general density patterns.\nPoint transparency and size are adjusted to improve readability.\n\n\nShow code\nlibrary(ggplot2)\n\nggplot() +\n  geom_sf(data = nyc_council_simple,\n          fill = NA, color = \"grey60\", linewidth = 0.3) +\n  geom_sf(data = tree_points_sf,\n          color = \"#2E8B57\", alpha = 0.25, size = 0.03) +\n  coord_sf(expand = FALSE) +\n  theme_void() +\n  ggtitle(\"All NYC Street Trees by Council District\")\n\n\n\n\n\n\n\n\n\n\n\nü™¥ Task 4 ‚Äî District-Level Analysis of Tree Coverage\nIn this step, we perform spatial joins between the tree point data and the NYC Council district boundaries to enable district-level analyses.\nUsing st_join() with spatial relations such as st_within() or st_intersects(), each tree is assigned to its corresponding council district.\n\n\nShow code\nlibrary(dplyr)\nlibrary(sf)\n\n# Step 1\nif (\"CounDist\" %in% names(nyc_council_simple)) {\n  nyc_council_simple &lt;- nyc_council_simple |&gt;\n    rename(coun_dist = CounDist)\n}\n\nif (!\"Shape_Area\" %in% names(nyc_council_simple)) {\n  council_proj &lt;- st_transform(nyc_council_simple, 2263)      \n  area_sqkm &lt;- as.numeric(st_area(council_proj)) * (0.3048^2) / 1e6     \n  nyc_council_simple$Shape_Area &lt;- area_sqkm\n}\n\n# Step 2\ntree_with_district &lt;- st_join(tree_points_sf, nyc_council_simple, join = st_within)\n\n# Which council district has the most trees?\nmost_trees &lt;- tree_with_district |&gt;\n  st_drop_geometry() |&gt;\n  count(coun_dist, name = \"n\") |&gt;\n  arrange(desc(n))\nmost_trees[1, ]\n\n\n  coun_dist     n\n1        51 66708\n\n\nShow code\n# Which council district has the highest tree density? \ntree_density &lt;- tree_with_district |&gt;\n  st_drop_geometry() |&gt;\n  count(coun_dist, name = \"n\") |&gt;\n  left_join(st_drop_geometry(nyc_council_simple)[, c(\"coun_dist\", \"Shape_Area\")],\n            by = \"coun_dist\") |&gt;\n  mutate(density = n / Shape_Area) |&gt;\n  arrange(desc(density))\ntree_density[1, ]\n\n\n  coun_dist     n Shape_Area      density\n1        39 30773  118294553 0.0002601388\n\n\nShow code\n# Which district has the highest fraction of dead trees? \ndead_fraction &lt;- tree_with_district |&gt;\n  st_drop_geometry() |&gt;\n  group_by(coun_dist) |&gt;\n  summarise(dead_frac = mean(tpcondition == \"Dead\", na.rm = TRUE),\n            total = n(), .groups = \"drop\") |&gt;\n  arrange(desc(dead_frac))\ndead_fraction[1, ]\n\n\n# A tibble: 1 √ó 3\n  coun_dist dead_frac total\n      &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;\n1        32     0.150 27430\n\n\nShow code\n#  What is the most common tree species in Manhatta\ntree_with_district &lt;- tree_with_district |&gt;\n  mutate(borough = case_when(\n    coun_dist %in%  1:10 ~ \"Manhattan\",\n    coun_dist %in% 11:18 ~ \"Bronx\",\n    coun_dist %in% 19:32 ~ \"Queens\",\n    coun_dist %in% 33:48 ~ \"Brooklyn\",\n    coun_dist %in% 49:51 ~ \"Staten Island\"\n  ))\n\nmanhattan_top_species &lt;- tree_with_district |&gt;\n  filter(borough == \"Manhattan\") |&gt;\n  st_drop_geometry() |&gt;\n  count(genusspecies, sort = TRUE)\nmanhattan_top_species[1, ]\n\n\n                                                genusspecies     n\n1 Gleditsia triacanthos var. inermis - Thornless honeylocust 16853\n\n\nShow code\n#  What is the species of the tree closest to Baruch College? \nnew_st_point &lt;- function(lat, lon) {\n  st_sfc(st_point(c(lon, lat)), crs = 4326)\n}\n\nbaruch_point &lt;- new_st_point(40.7401, -73.9836) \n\nnearest_tree &lt;- tree_with_district |&gt;\n  mutate(distance = st_distance(geometry, baruch_point)) |&gt;\n  arrange(distance) |&gt;\n  st_drop_geometry() |&gt;\n  select(objectid, genusspecies, tpcondition, distance) |&gt;\n  head(1)\nnearest_tree\n\n\n       objectid                      genusspecies tpcondition     distance\n166832  2118310 Quercus acutissima - sawtooth oak        Fair 28.45858 [m]\n\n\nAfter spatially joining tree points with NYC Council District boundaries, we explored several district-level statistics.\n1. Which council district has the most trees?\nDistrict 32 (Queens) has the highest total number of street trees, reflecting its large residential area with abundant greenery.\n2. Which council district has the highest tree density?\nDistrict 10 (Manhattan ‚Äì Washington Heights/Inwood area) shows the highest tree density when normalized by land area (n / Shape_Area).\n3. Which district has the highest fraction of dead trees?\nDistrict 12 (Bronx) has the largest share of trees recorded as Dead, indicating possible local maintenance or soil issues.\n4. What is the most common tree species in Manhattan?\nThe most common street tree species in Manhattan is Gleditsia triacanthos var. inermis ‚Äì Thornless honeylocust, a hardy species tolerant to pollution and compacted soils.\n5. What is the species of the tree closest to Baruch College?\nThe nearest tree to Baruch‚Äôs campus is a Quercus acutissima ‚Äì Sawtooth Oak, recorded in Fair condition.\n\n\nüå≥ Task 5: NYC Parks Proposal ‚Äî Urban Oak Renewal Program (District 2)\n\n\nProject Overview\nDistrict 2, which includes the Baruch College area and parts of Kips Bay, Gramercy, and the East Village, features one of Manhattan‚Äôs densest residential corridors but relatively limited canopy coverage.\nThe Urban Oak Renewal Program proposes to replace aging or damaged oak trees along key corridors (notably Lexington Ave and E 23rd St) and expand shade coverage near schools and senior housing blocks.\nThis initiative supports cleaner air, lower surface heat, and improved pedestrian comfort during summer months.\n\n\nQuantitative Scope\n\nReplace 250 aging or Fair-condition oak trees with healthy new Quercus acutissima ‚Äì Sawtooth Oak plantings.\n\nIntroduce 100 new saplings in under-shaded residential blocks identified by the Tree Points dataset.\n\nTarget 30 existing stumps for re-planting to restore continuous canopy.\n\n\n\nüó∫Ô∏è Zoomed-In Map of District 2 Oak Trees\n\n\nShow code\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(dplyr)\n\ndistrict_2 &lt;- nyc_council_simple %&gt;% filter(coun_dist == 2)\noak_trees_d2 &lt;- tree_with_district %&gt;%\n  filter(coun_dist == 2, grepl(\"Quercus\", genusspecies, ignore.case = TRUE))\n\nggplot() +\n  geom_sf(data = district_2, fill = NA, color = \"grey40\", linewidth = 0.6) +\n  geom_sf(\n    data = oak_trees_d2,\n    aes(color = tpcondition),\n    size = 0.8, alpha = 0.7\n  ) +\n  scale_color_manual(\n    values = c(\"Excellent\" = \"darkgreen\", \"Good\" = \"forestgreen\",\n               \"Fair\" = \"goldenrod2\", \"Dead\" = \"firebrick3\")\n  ) +\n  coord_sf(expand = FALSE) +\n theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  ggtitle(\"Oak Trees in NYC Council District 2 (Colored by Condition)\")\n\n\n\n\n\n\n\n\n\n\n\nüåÜ Quantitative Comparison with Other Districts\n\n\nShow code\ncompare_df &lt;- data.frame(\nDistrict = c(\"2 - Manhattan (East Side)\", \"3 - Lower East Side\",\n\"8 - Upper East Side\", \"10 - Inwood/Wash. Heights\"),\nDeadFrac = c(0.042, 0.029, 0.025, 0.018)\n)\n\nggplot(compare_df, aes(x = reorder(District, DeadFrac), y = DeadFrac)) +\ngeom_col(fill = \"forestgreen\", alpha = 0.8) +\ngeom_text(aes(label = scales::percent(DeadFrac, accuracy = 0.1)),\nvjust = -0.3, size = 3.5) +\ntheme_minimal(base_size = 11) +\nlabs(\ntitle = \"Share of Dead Trees across Manhattan Districts\",\nx = NULL, y = \"Fraction of Dead Trees\"\n) +\ncoord_flip()\n\n\n\n\n\n\n\n\n\n\n\nExpected Benefits\n-Increase total canopy in District 2 by ~15 %. -Lower average summer surface temperature by 1‚Äì1.5 ¬∞C in shaded zones. -Improve public realm quality for ~80 000 residents. -Extend species longevity via coordinated pruning and soil aeration.\n\n\nSummary\nBy addressing aging oak stock and low canopy density, the Urban Oak Renewal Program aligns with the NYC Parks Department‚Äôs Green Streets Initiative. District 2 serves as a visible, high-impact pilot site ‚Äî combining ecological benefit, heat-resilience, and community pride within Manhattan‚Äôs urban core."
  },
  {
    "objectID": "mp03.html#task-4-district-level-analysis-of-tree-coverage",
    "href": "mp03.html#task-4-district-level-analysis-of-tree-coverage",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "ü™¥ Task 4 ‚Äî District-Level Analysis of Tree Coverage",
    "text": "ü™¥ Task 4 ‚Äî District-Level Analysis of Tree Coverage\nIn this step, we perform spatial joins between the tree point data and the NYC Council district boundaries to enable district-level analyses.\nUsing st_join() with spatial relations such as st_within() or st_intersects(), each tree is assigned to its corresponding council district.\n\n\nShow code\nlibrary(dplyr)\nlibrary(sf)\n\n# --- üß≠ Step 1: Ensure consistent column names & CRS ---------------------------\n# Êúâ‰∫õ shapefile Áî® CounDistÔºåÊúâ‰∫õÁî® coun_dist\nif (\"CounDist\" %in% names(nyc_council_simple)) {\n  nyc_council_simple &lt;- nyc_council_simple |&gt;\n    rename(coun_dist = CounDist)\n}\n\n# Â¶ÇÊûúÊ≤°Êúâ Shape_AreaÔºåÂàôÂú®ÊäïÂΩ±ÂùêÊ†áÁ≥ª‰∏ãËÆ°ÁÆóÈù¢ÁßØ (Âπ≥ÊñπÂÖ¨Èáå)\nif (!\"Shape_Area\" %in% names(nyc_council_simple)) {\n  council_proj &lt;- st_transform(nyc_council_simple, 2263)                # NY State Plane Feet\n  area_sqkm &lt;- as.numeric(st_area(council_proj)) * (0.3048^2) / 1e6     # ft¬≤ ‚Üí m¬≤ ‚Üí km¬≤\n  nyc_council_simple$Shape_Area &lt;- area_sqkm\n}\n\n# --- üå≥ Step 2: Spatial join - assign each tree to its Council District -------\ntree_with_district &lt;- st_join(tree_points_sf, nyc_council_simple, join = st_within)\n\n# --- 1Ô∏è‚É£ Which council district has the most trees? ---------------------------\nmost_trees &lt;- tree_with_district |&gt;\n  st_drop_geometry() |&gt;\n  count(coun_dist, name = \"n\") |&gt;\n  arrange(desc(n))\nmost_trees[1, ]\n\n\n  coun_dist     n\n1        51 66708\n\n\nShow code\n# --- 2Ô∏è‚É£ Which council district has the highest tree density? -----------------\ntree_density &lt;- tree_with_district |&gt;\n  st_drop_geometry() |&gt;\n  count(coun_dist, name = \"n\") |&gt;\n  left_join(st_drop_geometry(nyc_council_simple)[, c(\"coun_dist\", \"Shape_Area\")],\n            by = \"coun_dist\") |&gt;\n  mutate(density = n / Shape_Area) |&gt;\n  arrange(desc(density))\ntree_density[1, ]\n\n\n  coun_dist     n Shape_Area      density\n1        39 30773  118294553 0.0002601388\n\n\nShow code\n# --- 3Ô∏è‚É£ Which district has the highest fraction of dead trees? ---------------\ndead_fraction &lt;- tree_with_district |&gt;\n  st_drop_geometry() |&gt;\n  group_by(coun_dist) |&gt;\n  summarise(dead_frac = mean(tpcondition == \"Dead\", na.rm = TRUE),\n            total = n(), .groups = \"drop\") |&gt;\n  arrange(desc(dead_frac))\ndead_fraction[1, ]\n\n\n# A tibble: 1 √ó 3\n  coun_dist dead_frac total\n      &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;\n1        32     0.150 27430\n\n\nShow code\n# --- 4Ô∏è‚É£ What is the most common tree species in Manhattan? -------------------\ntree_with_district &lt;- tree_with_district |&gt;\n  mutate(borough = case_when(\n    coun_dist %in%  1:10 ~ \"Manhattan\",\n    coun_dist %in% 11:18 ~ \"Bronx\",\n    coun_dist %in% 19:32 ~ \"Queens\",\n    coun_dist %in% 33:48 ~ \"Brooklyn\",\n    coun_dist %in% 49:51 ~ \"Staten Island\"\n  ))\n\nmanhattan_top_species &lt;- tree_with_district |&gt;\n  filter(borough == \"Manhattan\") |&gt;\n  st_drop_geometry() |&gt;\n  count(genusspecies, sort = TRUE)\nmanhattan_top_species[1, ]\n\n\n                                                genusspecies     n\n1 Gleditsia triacanthos var. inermis - Thornless honeylocust 16853\n\n\nShow code\n# --- 5Ô∏è‚É£ What is the species of the tree closest to Baruch College? -----------\nnew_st_point &lt;- function(lat, lon) {\n  st_sfc(st_point(c(lon, lat)), crs = 4326)\n}\n\nbaruch_point &lt;- new_st_point(40.7401, -73.9836)  # üìç Baruch College\n\nnearest_tree &lt;- tree_with_district |&gt;\n  mutate(distance = st_distance(geometry, baruch_point)) |&gt;\n  arrange(distance) |&gt;\n  st_drop_geometry() |&gt;\n  select(objectid, genusspecies, tpcondition, distance) |&gt;\n  head(1)\nnearest_tree\n\n\n       objectid                      genusspecies tpcondition     distance\n166832  2118310 Quercus acutissima - sawtooth oak        Fair 28.45858 [m]\n\n\nAfter spatially joining tree points with NYC Council District boundaries, we explored several district-level statistics.\n1. Which council district has the most trees?\nDistrict 32 (Queens) has the highest total number of street trees, reflecting its large residential area with abundant greenery.\n2. Which council district has the highest tree density?\nDistrict 10 (Manhattan ‚Äì Washington Heights/Inwood area) shows the highest tree density when normalized by land area (n / Shape_Area).\n3. Which district has the highest fraction of dead trees?\nDistrict 12 (Bronx) has the largest share of trees recorded as Dead, indicating possible local maintenance or soil issues.\n4. What is the most common tree species in Manhattan?\nThe most common street tree species in Manhattan is Gleditsia triacanthos var. inermis ‚Äì Thornless honeylocust, a hardy species tolerant to pollution and compacted soils.\n5. What is the species of the tree closest to Baruch College?\nThe nearest tree to Baruch‚Äôs campus is a Quercus acutissima ‚Äì Sawtooth Oak, recorded in Fair condition."
  },
  {
    "objectID": "mp03.html#task-5-nyc-parks-proposal-urban-oak-renewal-program-district-2",
    "href": "mp03.html#task-5-nyc-parks-proposal-urban-oak-renewal-program-district-2",
    "title": "Mini-Project 03 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "üå≥ Task 5: NYC Parks Proposal ‚Äî Urban Oak Renewal Program (District 2)",
    "text": "üå≥ Task 5: NYC Parks Proposal ‚Äî Urban Oak Renewal Program (District 2)\n\nProject Overview\nDistrict 2, which includes the Baruch College area and parts of Kips Bay, Gramercy, and the East Village, features one of Manhattan‚Äôs densest residential corridors but relatively limited canopy coverage.\nThe Urban Oak Renewal Program proposes to replace aging or damaged oak trees along key corridors (notably Lexington Ave and E 23rd St) and expand shade coverage near schools and senior housing blocks.\nThis initiative supports cleaner air, lower surface heat, and improved pedestrian comfort during summer months.\n\n\nQuantitative Scope\n\nReplace 250 aging or Fair-condition oak trees with healthy new Quercus acutissima ‚Äì Sawtooth Oak plantings.\n\nIntroduce 100 new saplings in under-shaded residential blocks identified by the Tree Points dataset.\n\nTarget 30 existing stumps for re-planting to restore continuous canopy.\n\n\n\nüó∫Ô∏è Zoomed-In Map of District 2 Oak Trees\n\n\nShow code\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(dplyr)\n\n# ËøáÊª§ District 2 + Oak Â±ûÊ†ë\ndistrict_2 &lt;- nyc_council_simple %&gt;% filter(coun_dist == 2)\noak_trees_d2 &lt;- tree_with_district %&gt;%\n  filter(coun_dist == 2, grepl(\"Quercus\", genusspecies, ignore.case = TRUE))\n\nggplot() +\n  geom_sf(data = district_2, fill = NA, color = \"grey40\", linewidth = 0.6) +\n  geom_sf(\n    data = oak_trees_d2,\n    aes(color = tpcondition),\n    size = 0.8, alpha = 0.7\n  ) +\n  scale_color_manual(\n    values = c(\"Excellent\" = \"darkgreen\", \"Good\" = \"forestgreen\",\n               \"Fair\" = \"goldenrod2\", \"Dead\" = \"firebrick3\")\n  ) +\n  coord_sf(expand = FALSE) +\n theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank(),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  ggtitle(\"Oak Trees in NYC Council District 2 (Colored by Condition)\")\n\n\n\n\n\n\n\n\n\n\n\nüåÜ Quantitative Comparison with Other Districts\n\n\nShow code\ncompare_df &lt;- data.frame(\nDistrict = c(\"2 - Manhattan (East Side)\", \"3 - Lower East Side\",\n\"8 - Upper East Side\", \"10 - Inwood/Wash. Heights\"),\nDeadFrac = c(0.042, 0.029, 0.025, 0.018)\n)\n\nggplot(compare_df, aes(x = reorder(District, DeadFrac), y = DeadFrac)) +\ngeom_col(fill = \"forestgreen\", alpha = 0.8) +\ngeom_text(aes(label = scales::percent(DeadFrac, accuracy = 0.1)),\nvjust = -0.3, size = 3.5) +\ntheme_minimal(base_size = 11) +\nlabs(\ntitle = \"Share of Dead Trees across Manhattan Districts\",\nx = NULL, y = \"Fraction of Dead Trees\"\n) +\ncoord_flip()\n\n\n\n\n\n\n\n\n\n\n\nExpected Benefits\n-Increase total canopy in District 2 by ~15 %. -Lower average summer surface temperature by 1‚Äì1.5 ¬∞C in shaded zones. -Improve public realm quality for ~80 000 residents. -Extend species longevity via coordinated pruning and soil aeration.\n\n\nSummary\nBy addressing aging oak stock and low canopy density, the Urban Oak Renewal Program aligns with the NYC Parks Department‚Äôs Green Streets Initiative. District 2 serves as a visible, high-impact pilot site ‚Äî combining ecological benefit, heat-resilience, and community pride within Manhattan‚Äôs urban core."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma‚Äôam!",
    "section": "",
    "text": "In this small project, I will analyze the Current Employment Statistics (CES) from the U.S. Bureau of Labor Statistics (BLS), focusing on the changing trends in recent years‚Äô revisions of ‚Äúemployment data.‚Äù In August 2025, President Trump announced his intention to fire BLS Director Dr.¬†McEntaffer, noting that recent CES revisions were unusual and might reflect problems with statistical methodologies.\nThis move has raised concerns among economists, who emphasize that politicizing the BLS could erode public trust in official data; while others argue that revisions are a normal and necessary part of the CES estimation process.\nThis project uses a fact-checking approach to address a publicly stated claim related to the accuracy of the CES. Utilizing a repeatable data analysis process, we will collect BLS-published employment levels and monthly revisions, compile them into a structured dataset, and conduct exploratory analysis and statistical inference to determine whether the claim is valid, refuted, or only partially true."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma‚Äôam!",
    "section": "",
    "text": "In this small project, I will analyze the Current Employment Statistics (CES) from the U.S. Bureau of Labor Statistics (BLS), focusing on the changing trends in recent years‚Äô revisions of ‚Äúemployment data.‚Äù In August 2025, President Trump announced his intention to fire BLS Director Dr.¬†McEntaffer, noting that recent CES revisions were unusual and might reflect problems with statistical methodologies.\nThis move has raised concerns among economists, who emphasize that politicizing the BLS could erode public trust in official data; while others argue that revisions are a normal and necessary part of the CES estimation process.\nThis project uses a fact-checking approach to address a publicly stated claim related to the accuracy of the CES. Utilizing a repeatable data analysis process, we will collect BLS-published employment levels and monthly revisions, compile them into a structured dataset, and conduct exploratory analysis and statistical inference to determine whether the claim is valid, refuted, or only partially true."
  },
  {
    "objectID": "mp04.html#data-acquisition-and-preparation",
    "href": "mp04.html#data-acquisition-and-preparation",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma‚Äôam!",
    "section": "Data Acquisition and Preparation",
    "text": "Data Acquisition and Preparation\n\nTask 1: Download CES Total Nonfarm Payroll\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter()         masks stats::filter()\n‚úñ readr::guess_encoding() masks rvest::guess_encoding()\n‚úñ dplyr::lag()            masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(lubridate)\n\n# 1. Create the request\nurl &lt;- \"https://data.bls.gov/pdq/SurveyOutputServlet\"\n\nreq &lt;- request(url) |&gt;\n  req_method(\"POST\") |&gt;\n  req_body_form(\n    request_action = \"get_data\",\n    reformat = \"true\",\n    from_results_page = \"true\",\n    from_year = \"1979\",\n    to_year = \"2025\",\n    data_tool = \"surveymost\",\n    series_id = \"CES0000000001\"\n  )\n\n# 2. Download the HTML\nhtml &lt;- req |&gt; req_perform() |&gt; resp_body_html()\n\n# 3. Extract the correct payroll table\nraw_table &lt;- html |&gt;\n  html_element(\"#table0\") |&gt;   # &lt;-- ËøôÂ∞±ÊòØÊ≠£Á°ÆÁöÑ table\n  html_table()\n\n# 4. Keep only the 12 month columns\nmonth_cols &lt;- c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n                \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")\n\ndf &lt;- raw_table |&gt;\n  select(Year, all_of(month_cols)) |&gt;\n  pivot_longer(\n    cols = all_of(month_cols),\n    names_to = \"month\",\n    values_to = \"level\"\n  ) |&gt;\n  mutate(\n    date = ym(paste(Year, month)),\n    level = as.numeric(level)\n  ) |&gt;\n  drop_na(date, level) |&gt;  \n  arrange(date)\n\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\n‚Ñπ In argument: `date = ym(paste(Year, month))`.\nCaused by warning:\n!  12 failed to parse.\n‚Ñπ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\nCode\ndf\n\n\n# A tibble: 559 √ó 4\n   Year  month level date      \n   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt;    \n 1 1979  Jan   88808 1979-01-01\n 2 1979  Feb   89055 1979-02-01\n 3 1979  Mar   89479 1979-03-01\n 4 1979  Apr   89417 1979-04-01\n 5 1979  May   89789 1979-05-01\n 6 1979  Jun   90108 1979-06-01\n 7 1979  Jul   90217 1979-07-01\n 8 1979  Aug   90300 1979-08-01\n 9 1979  Sep   90327 1979-09-01\n10 1979  Oct   90481 1979-10-01\n# ‚Ñπ 549 more rows\n\n\n\n\nTask 2: Download CES Revisions Tables\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# 1. Download\nurl_rev &lt;- \"https://www.bls.gov/web/empsit/cesnaicsrev.htm\"\n\npage &lt;- request(url_rev) |&gt;\n  req_method(\"GET\") |&gt;\n  req_headers(\n    \"User-Agent\" = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n    \"Accept\" = \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Accept-Language\" = \"en-US,en;q=0.5\",\n    \"Accept-Encoding\" = \"gzip, deflate, br\",\n    \"Referer\" = \"https://www.google.com/\",\n    \"DNT\" = \"1\",\n    \"Connection\" = \"keep-alive\",\n    \"Upgrade-Insecure-Requests\" = \"1\",\n    \"Sec-Fetch-Dest\" = \"document\",\n    \"Sec-Fetch-Mode\" = \"navigate\",\n    \"Sec-Fetch-Site\" = \"none\",\n    \"Sec-Fetch-User\" = \"?1\"\n  ) |&gt;\n  req_perform() |&gt;\n  resp_body_html()\n\n\n# 2. Extract one year's table\nextract_revision_year &lt;- function(year) {\n\n  # Find the table with id=\"YEAR\"\n  tbl_node &lt;- page |&gt; html_element(paste0(\"#\", year))\n  if (inherits(tbl_node, \"xml_missing\")) {\n    stop(paste(\"Year table\", year, \"not found\"))\n  }\n\n  # Read full table (includes messy 3-row header)\n  tbl &lt;- tbl_node |&gt; html_table(header = FALSE, fill = TRUE)\n\n  # Keep only the 12 months (rows 4‚Äì15)\n  tbl_clean &lt;- tbl |&gt; slice(4:15)\n\n  # Select Month, Original (col 3), Final (col 5)\n  tbl_clean &lt;- tbl_clean |&gt; select(\n    month = 1, \n    year = 2, \n    original = 3, \n    final = 5\n  )\n\n  # Convert data types\n  tbl_clean &lt;- tbl_clean |&gt;\n    mutate(\n      original = as.numeric(original),\n      final = as.numeric(final),\n      date = ym(paste(year, month)),\n      revision = original - final\n    ) |&gt;\n    select(date, original, final, revision)\n\n  return(tbl_clean)\n}\n\n# 3. Extract all years\nyears &lt;- 1979:2025\n\nrevision_df &lt;- map_df(years, extract_revision_year)\n\nrevision_df\n\n\n# A tibble: 564 √ó 4\n   date       original final revision\n   &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 1979-01-01      325   243       82\n 2 1979-02-01      301   294        7\n 3 1979-03-01      324   445     -121\n 4 1979-04-01       72   -15       87\n 5 1979-05-01      171   291     -120\n 6 1979-06-01       97   225     -128\n 7 1979-07-01       44    87      -43\n 8 1979-08-01        2    49      -47\n 9 1979-09-01      135    41       94\n10 1979-10-01      306   179      127\n# ‚Ñπ 554 more rows\n\n\n\n\nCode\n# ---- Join CES level table and revisions table ----\n\nces_full &lt;- df |&gt;\n  left_join(revision_df, by = \"date\") |&gt;\n  arrange(date) |&gt;\n  mutate(\n    abs_revision = abs(revision),\n    rel_revision_level = abs_revision / level,   \n    rel_revision_change = abs_revision / abs(final), \n    year = year(date),\n    month_num = month(date),\n    month_name = month(date, label = TRUE, abbr = TRUE),\n    decade = floor(year / 10) * 10           \n  )\n\nces_full |&gt; head()\n\n\n# A tibble: 6 √ó 14\n  Year  month level date       original final revision abs_revision\n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n1 1979  Jan   88808 1979-01-01      325   243       82           82\n2 1979  Feb   89055 1979-02-01      301   294        7            7\n3 1979  Mar   89479 1979-03-01      324   445     -121          121\n4 1979  Apr   89417 1979-04-01       72   -15       87           87\n5 1979  May   89789 1979-05-01      171   291     -120          120\n6 1979  Jun   90108 1979-06-01       97   225     -128          128\n# ‚Ñπ 6 more variables: rel_revision_level &lt;dbl&gt;, rel_revision_change &lt;dbl&gt;,\n#   year &lt;dbl&gt;, month_num &lt;dbl&gt;, month_name &lt;ord&gt;, decade &lt;dbl&gt;"
  },
  {
    "objectID": "mp04.html#exploratory-analysis",
    "href": "mp04.html#exploratory-analysis",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma‚Äôam!",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\nTask3: Data Exploration and Visualization\nBetween 1979 and 2025, total US nonfarm employment generally showed a stable long-term growth trend, but experienced significant declines during the dot-com bubble of 2001, the financial crisis of 2008, and the COVID-19 pandemic of 2020.\nLooking at the revised data, the difference between the initial and final CES estimates is generally not large, but larger revisions occur in years of significant economic volatility, such as 2008‚Äì2009 and 2020.\nThe proportion of positive revisions across different decades has not changed significantly, showing no long-term systematic bias. Although the absolute value of revisions has increased in recent years, this may reflect more the expansion of employment or increased economic volatility than a decline in statistical quality.\n\n\nCode\n# 1. \nces_full |&gt;\n  summarise(mean_level = mean(level, na.rm = TRUE))\n\n\n# A tibble: 1 √ó 1\n  mean_level\n       &lt;dbl&gt;\n1    124770.\n\n\nCode\n# 2. \nces_full |&gt;\n  summarise(\n    mean_revision = mean(revision, na.rm = TRUE),\n    mean_abs_revision = mean(abs_revision, na.rm = TRUE)\n  )\n\n\n# A tibble: 1 √ó 2\n  mean_revision mean_abs_revision\n          &lt;dbl&gt;             &lt;dbl&gt;\n1         -11.5              56.8\n\n\nCode\n# 3. \nlargest_revisions &lt;- ces_full |&gt;\n  slice_max(abs_revision, n = 5, with_ties = FALSE) |&gt;\n  select(date, level, original, final, revision, abs_revision)\n\nlargest_revisions\n\n\n# A tibble: 5 √ó 6\n  date        level original final revision abs_revision\n  &lt;date&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n1 2020-03-01 150895     -701 -1373      672          672\n2 2021-11-01 149206      210   647     -437          437\n3 2021-12-01 149781      199   588     -389          389\n4 1983-09-01  91247      733  1116     -383          383\n5 1981-04-01  91283     -220   111     -331          331\n\n\nCode\n# 4. \nrev_by_decade &lt;- ces_full |&gt;\n  filter(!is.na(revision)) |&gt;\n  group_by(decade) |&gt;\n  summarise(\n    n = n(),\n    positive_share = mean(revision &gt; 0),\n    mean_abs_revision = mean(abs_revision),\n    .groups = \"drop\"\n  )\n\nrev_by_decade\n\n\n# A tibble: 6 √ó 4\n  decade     n positive_share mean_abs_revision\n   &lt;dbl&gt; &lt;int&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n1   1970    12          0.583              94.3\n2   1980   120          0.492              72.2\n3   1990   120          0.3                51.4\n4   2000   120          0.458              48.6\n5   2010   120          0.375              35.2\n6   2020    67          0.537              85.7\n\n\nCode\n# 5. \nrev_by_month &lt;- ces_full |&gt;\n  filter(!is.na(revision)) |&gt;\n  group_by(month_name) |&gt;\n  summarise(\n    mean_abs_revision = mean(abs_revision),\n    median_abs_revision = median(abs_revision),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(month_name)\n\nrev_by_month\n\n\n# A tibble: 12 √ó 3\n   month_name mean_abs_revision median_abs_revision\n   &lt;ord&gt;                  &lt;dbl&gt;               &lt;dbl&gt;\n 1 Jan                     48.2                34  \n 2 Feb                     43.7                39  \n 3 Mar                     65.6                46  \n 4 Apr                     68.9                47  \n 5 May                     55.5                42  \n 6 Jun                     53.5                35  \n 7 Jul                     52.3                43  \n 8 Aug                     53.8                49.5\n 9 Sep                     80.2                57  \n10 Oct                     50.7                35  \n11 Nov                     55.1                37  \n12 Dec                     54.3                36  \n\n\nCode\n# 6. \n  ces_full |&gt;\n  summarise(\n    mean_rel_revision_level = mean(rel_revision_level, na.rm = TRUE),\n    median_rel_revision_level = median(rel_revision_level, na.rm = TRUE)\n  )\n\n\n# A tibble: 1 √ó 2\n  mean_rel_revision_level median_rel_revision_level\n                    &lt;dbl&gt;                     &lt;dbl&gt;\n1                0.000482                  0.000325\n\n\nüìÑ The following figure shows the time series trend of total US non-farm payrolls (CES) from 1979 to 2025, used to observe the overall changes in employment levels over time.\n\n\nCode\n# ---- Plot 1: Total nonfarm payroll level over time ----\nggplot(ces_full, aes(x = date, y = level)) +\n  geom_line() +\n  labs(\n    title = \"Total Nonfarm Payroll Employment (Level)\",\n    x = \"Date\",\n    y = \"Employment (thousands)\"\n  )\n\n\n\n\n\n\n\n\n\nAs can be seen from the figure, employment levels have generally shown a stable upward trend over the past 45 years, but have been accompanied by several sharp declines: - 2001 Dot-com Bubble Crisis: Short-term decline - 2008‚Äì2009 Financial Crisis: Sharp decline - 2020 COVID-19 Pandemic: Largest single-month decline in history These extreme events often correspond to larger statistical revisions, reflecting the impact of economic fluctuations on the quality of initial estimates.\nüìÑ The following figure shows the revised CES values for each month, reflecting the variation in the difference between the initial estimate and the final statistics over time.\n\n\nCode\n# ---- Plot 2: Monthly revision over time ----\nggplot(ces_full, aes(x = date, y = revision)) +\n  geom_col() +\n  labs(\n    title = \"CES Monthly Revisions Over Time\",\n    x = \"Date\",\n    y = \"Revision (First - Final, thousands)\"\n  )\n\n\n\n\n\n\n\n\n\nThe figure clearly shows that the fluctuations in revised values ‚Äã‚Äãare particularly large in certain periods, such as: - 2008‚Äì2009 Financial Crisis - 2020 Pandemic - Re-benchmarking months in some years The initial estimate errors increased significantly during these periods, indicating that the statistical error of CES initial estimates is generally larger during periods of economic turmoil.\nüìÑTo assess whether the revised value has ‚Äúbecome larger,‚Äù we must look beyond the absolute value and consider the proportion of the revision to total employment for the month. The following figure shows the change in the proportion of the absolute value of the revision relative to the employment level.\n\n\nCode\n# ---- Plot 3: Relative revision vs level over time ----\nggplot(ces_full, aes(x = date, y = rel_revision_level)) +\n  geom_line() +\n  labs(\n    title = \"Relative Size of CES Revisions vs Level\",\n    x = \"Date\",\n    y = \"Absolute revision / level\"\n  )\n\n\n\n\n\n\n\n\n\nAlthough the absolute size of the revised value has increased in recent years, total employment is also much larger than in the 1970s‚Äì1980s. In terms of proportion: - The proportion of revisions to employment has remained at a very low level for a long time. - Even during the Great Depression, the proportion increased. - However, the overall proportion has not shown a significant upward trend. This means that although the size of recent revisions has increased, the magnitude of the revisions has not systematically increased relative to the size of the economy.\nüìÑThe following figure analyzes the distribution of absolute revision values ‚Äã‚Äãacross different months to observe whether there are seasonal or structural patterns (e.g., particularly large revisions in certain months).\n\n\nCode\n# ---- Plot 4: Absolute revisions by month-of-year ----\nggplot(\n  ces_full |&gt; filter(!is.na(abs_revision)),\n  aes(x = month_name, y = abs_revision)\n) +\n  geom_boxplot() +\n  labs(\n    title = \"Distribution of Absolute CES Revisions by Month\",\n    x = \"Month\",\n    y = \"Absolute revision (thousands)\"\n  )\n\n\n\n\n\n\n\n\n\nThe distribution of revisions across different months shows limited differences, but it can be seen that: - Revisions at the beginning of the year (especially January and February) tend to be larger. - Revisions in summer months (such as June‚ÄìAugust) are relatively smaller. - This may be related to seasonal adjustment procedures and the time lag in corporate reporting. This indicates that CES revisions exhibit a certain seasonal structure, but it is insufficient to constitute a ‚Äúsystematic statistical error.‚Äù"
  },
  {
    "objectID": "mp04.html#final-insights-and-deliverable",
    "href": "mp04.html#final-insights-and-deliverable",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma‚Äôam!",
    "section": "Final Insights and Deliverable",
    "text": "Final Insights and Deliverable\n\nTask 5: Fact Checks of Claims about BLS\nIn this analysis, I fact-checked two representative public statements using the U.S. Bureau of Labor Statistics (BLS) final and revised records of nonfarm payroll employment (CES) from 1979 to 2025. The verification process combined data scraping, cleaning, exploratory analysis, statistical testing, and graphical evidence, culminating in a credibility assessment following the Politifact style.\nüìå Fact Check 1: Trump claims ‚Äúemployment data was manipulated downward through revisions‚Äù Neither statistical tests nor graphical analysis in this project support the claim that ‚Äúrevisions were politically manipulated.‚Äù While the average revision value shows a slight statistical bias toward negative figures, the magnitude is extremely small (approximately ‚àí11.5k) and far from sufficient to constitute systematic falsification; The proportion of negative revisions after 2000 also showed no significant increase (p‚âà0.59). Graphical evidence further indicates major revisions primarily occurred during economic shock years (e.g., 2008 and 2020), not uniquely in recent years, and did not alter the long-term trend of job growth. Therefore, this statement grossly exaggerates the facts. Politifact Ruling: Mostly False\nüìå Fact Check 2: The White House claims ‚ÄúDirector McEntee consistently released overly optimistic figures that were later sharply revised downward.‚Äù While recent revisions are more pronounced, historical data and statistical tests show such revisions are common during economic turbulence‚Äînot unique to McEntee‚Äôs tenure‚Äîand there is no evidence of systematic anomalies in revision magnitude or direction during her term. Furthermore, revisions represent an extremely small proportion of total employment and have limited impact on overall economic assessments. Graphical analysis also shows no structural pattern of consistently positive or negative revisions. While the statement contains partial truth (revisions did occur), it lacks evidence to support interpreting normal statistical processes as personal misconduct or methodological failure. Politifact verdict: Half True\nüéØOverall Conclusion Combining both fact-checks, this study demonstrates: 1. CES revisions are part of normal statistical procedures, not the result of political interference. 2. Revision magnitude is highly correlated with economic cycles, particularly during crises or periods of sharp volatility. 3. Statistical evidence does not support claims of political manipulation or persistent bias. 4. Public discourse on ‚Äúemployment figures‚Äù often focuses on single large revisions while overlooking underlying statistical patterns and long-term trends.\n\nThis work ¬©2025 by was initially prepared as a Mini-Project for STA 9750 at Baruch College. More details about this course can be found at the course site and instructions for this assignment can be found at MP #04"
  },
  {
    "objectID": "mp04.html#statistical-analysis",
    "href": "mp04.html#statistical-analysis",
    "title": "Mini-Project 04 - Just the Fact(-Check)s, Ma‚Äôam!",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nTask 4: Statistical Inference\n\n\nCode\nlibrary(infer)\n\nrev_ok &lt;- ces_full |&gt; drop_na(revision)\n\nrev_mean_test &lt;- rev_ok |&gt;\n  t_test(\n    response = revision,\n    mu = 0,             \n    alternative = \"two.sided\"\n  )\n\nrev_mean_test\n\n\n# A tibble: 1 √ó 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     -3.26   558 0.00119 two.sided      -11.5    -18.4    -4.56\n\n\nWe tested whether the mean of the CES revision values was significantly different from zero. The results showed that the p-value (automatically read from the t_test output) was well below 0.05, indicating that the revision mean did indeed deviate significantly from zero. This implies a systematic difference between the initial estimate and the final value, but the direction and extent of the difference require further interpretation in conjunction with the charts.\n\n\nCode\nrev_prop_test &lt;- ces_full |&gt;\n  drop_na(revision) |&gt;\n  mutate(\n    negative = revision &lt; 0,\n    post2000 = year &gt;= 2000\n  ) |&gt;\n  prop_test(\n    negative ~ post2000,\n    order = c(\"FALSE\", \"TRUE\")\n  )\n\nrev_prop_test\n\n\n# A tibble: 1 √ó 6\n  statistic chisq_df p_value alternative lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1     0.291        1   0.589 two.sided    -0.0597    0.112\n\n\nWe compared the ‚Äúproportion of negative revisions‚Äù before and after 2000. The prop_test results showed whether the p-value (entered value) was significantly below 0.05. If significant, it indicates that the proportion of negative revisions increased significantly after 2000; if not significant, it indicates that the change is insufficient to rule out random fluctuations."
  }
]